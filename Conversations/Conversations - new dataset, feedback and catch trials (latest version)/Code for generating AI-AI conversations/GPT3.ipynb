{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2622cf4f",
   "metadata": {
    "id": "2622cf4f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa750d0",
   "metadata": {
    "id": "9fa750d0"
   },
   "outputs": [],
   "source": [
    "openai.api_key = \"here_add_your_key\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28aec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Load API key from environment\n",
    "load_dotenv(\"variables.env\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Create OpenAI client\n",
    "api_key = openai.OpenAI(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57278845",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello! Can you confirm my API is working?\"}],\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a8c24",
   "metadata": {
    "id": "c69a8c24",
    "outputId": "30a8f727-9d09-4853-add9-55a38535594a"
   },
   "outputs": [],
   "source": [
    "# set parameters of the conversation\n",
    "#model = \"text-davinci-002\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "#model = \"text-curie-001\"\n",
    "#model = \"davinci\"\n",
    "#model = \"curie\"\n",
    "\n",
    "import time\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))  # Secure way to set API key\n",
    "\n",
    "\n",
    "temperature = 0.8\n",
    "temperature_regen = 1\n",
    "frequency_penalty = 2\n",
    "presence_penalty = 2\n",
    "\n",
    "list_names1 = ['John','Alice','Mark','Andrew','Julia', 'Georgia', 'Juliet', 'Lily', 'Olivia', 'Emmett', 'Miles', 'Oscar', 'William']\n",
    "list_names2 = ['Frank','Tom','Elizabeth','Gabriel','Paul', 'Alfie', 'Christina', 'Edward', 'Ella', 'Oliver', 'Karen', 'Isabel', 'Brad']\n",
    "list_topics = ['fashion', 'politics', 'books', 'sports', 'general entertainment', 'music', 'science', 'technology', 'movies', 'food']\n",
    "\n",
    "for conv in range(1):      #10-150\n",
    "    filename = 'conv'+str(conv)+'.html'\n",
    "    file_html = open(filename, \"w\")\n",
    "    \n",
    "    friend1 = np.random.choice(list_names1)\n",
    "    friend2 = np.random.choice(list_names2)\n",
    "    topic = np.random.choice(list_topics)\n",
    "    \n",
    "    prompt = friend1+\" greets \" +friend2+\". \"+friend2+\" starts to talk about \"+topic+\". Both ask long questions, give long responses and often disagree. Then the topic changes. The conversation never ends. \\n\"+friend1+\": Hi! \\n\"+friend2+\":\"\n",
    "\n",
    "    for i in range(12):\n",
    "        #print(\"________\",\"prompt\",i,\"A __________\")\n",
    "        #print(prompt)\n",
    "        num_words = 0\n",
    "        cnt = 0\n",
    "        while num_words < 5 and cnt<6:\n",
    "            #print(\"generate\")\n",
    "            if cnt<3:\n",
    "                temperature_step = temperature\n",
    "            else:\n",
    "                temperature_step = temperature_regen\n",
    "\n",
    "                time.sleep(1)\n",
    "                \n",
    "            response = client.completions.create(\n",
    "                model=model,\n",
    "                prompt=prompt,\n",
    "                temperature=temperature_step,\n",
    "                max_tokens=60,\n",
    "                top_p=1.0,\n",
    "                frequency_penalty=frequency_penalty,\n",
    "                presence_penalty=presence_penalty,\n",
    "                stop=[friend1+\":\",friend2+\":\"]\n",
    "            )\n",
    "            string = string = response.choices[0].text\n",
    "\n",
    "            word_list = string.split()\n",
    "            num_words = len(word_list)\n",
    "            cnt+=1\n",
    "            \n",
    "        speaker = \"\\n\"+friend1+\":\"\n",
    "        #print(speaker+ response[\"choices\"][0][\"text\"][0:])\n",
    "        \n",
    "        new_line = \"<p> A: \"+response[\"choices\"][0][\"text\"][0:]+\" </p>\\n\"\n",
    "        file_html.write(new_line)\n",
    "        \n",
    "        prompt += response[\"choices\"][0][\"text\"][0:]+speaker\n",
    "\n",
    "        #print(\"________\",\"prompt\",i,\"B __________\")\n",
    "        #print(prompt)\n",
    "        num_words = 0\n",
    "        cnt = 0\n",
    "        while num_words < 5 and cnt<6:\n",
    "            #print(\"generate\")\n",
    "            if cnt<3:\n",
    "                temperature_step = temperature\n",
    "            else:\n",
    "                temperature_step = temperature_regen\n",
    "            response = client.completions.create(\n",
    "                model=model,\n",
    "                prompt=prompt,\n",
    "                temperature=temperature_step,\n",
    "                max_tokens=60,\n",
    "                top_p=1.0,\n",
    "                frequency_penalty=frequency_penalty,\n",
    "                presence_penalty=presence_penalty,\n",
    "                stop=[friend1+\":\",friend2+\":\"]\n",
    "            )\n",
    "            string = response[\"choices\"][0][\"text\"][0:]\n",
    "            word_list = string.split()\n",
    "            num_words = len(word_list)\n",
    "            cnt+=1\n",
    "            \n",
    "        speaker = \"\\n\"+friend2+\":\"\n",
    "        #print(speaker+ response[\"choices\"][0][\"text\"][0:])\n",
    "        new_line = \"<p> B: \"+response[\"choices\"][0][\"text\"][0:]+\" </p>\\n\"\n",
    "        file_html.write(new_line)\n",
    "        \n",
    "        prompt += response[\"choices\"][0][\"text\"][0:]+speaker\n",
    "    print(prompt)\n",
    "    file_html.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb4a6f",
   "metadata": {},
   "source": [
    "### Elisa's code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import numpy as np  \n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load environment variables from the same folder\n",
    "load_dotenv(\"variables.env\")\n",
    "\n",
    "# Retrieve the API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# Set up OpenAI client with API key\n",
    "model = \"gpt-4o\"\n",
    "#model = \"deepseek-reasoner\"\n",
    "\n",
    "# Initialize API client based on the selected model\n",
    "if model.startswith(\"gpt-\"):  # Any OpenAI model (gpt-4o, gpt-4-turbo, etc.)\n",
    "    client = openai.OpenAI(api_key=openai_api_key)\n",
    "\n",
    "elif model.startswith(\"deepseek-\"):  # Any DeepSeek model (deepseek-chat, deepseek-code, etc.)\n",
    "    client = openai.OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Model '{model}' not recognized! Use 'gpt-4o' or 'deepseek-chat'.\")\n",
    "client = openai.OpenAI(api_key=api_key)  # Secure way to set API key\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba2cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bd176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.Client()\n",
    "\n",
    "models = client.models.list()\n",
    "print([model.id for model in models])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c67a7d",
   "metadata": {},
   "source": [
    "I extract the distribution of the human messages length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f36681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder with Giorgia's conversations\n",
    "html_folder = \"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/conversation_task/conversation_dataset\"  # Change this to your actual folder\n",
    "\n",
    "def extract_word_counts_from_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "        \n",
    "        # Extract text from all <p> tags\n",
    "        conversation_turns = soup.find_all(\"p\")\n",
    "        \n",
    "        word_counts = [len(turn.get_text().split()) for turn in conversation_turns]\n",
    "        return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9492fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_counts = []\n",
    "for i in range(1, 41):  # Load only conv1.html to conv40.html\n",
    "    html_file = os.path.join(html_folder, f\"conv{i}.html\")\n",
    "    if os.path.exists(html_file):\n",
    "        all_word_counts.extend(extract_word_counts_from_html(html_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72659c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_length():\n",
    "    return np.random.choice(all_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53bc5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_response_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d232e2",
   "metadata": {
    "id": "37d232e2"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import numpy as np  \n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "\n",
    "# Set parameters of the conversation\n",
    "#model = \"gpt-3.5-turbo\"\n",
    "#model = \"gpt-4o\"\n",
    "\n",
    "\n",
    "temperature = 0.6\n",
    "temperature_regen = 1\n",
    "frequency_penalty = 2\n",
    "presence_penalty = 2\n",
    "\n",
    "list_names1 = ['John','Alice','Mark','Andrew','Julia', 'Georgia', 'Juliet', 'Lily', 'Olivia', 'Emmett', 'Miles', 'Oscar', 'William']\n",
    "list_names2 = ['Frank','Tom','Elizabeth','Gabriel','Paul', 'Alfie', 'Christina', 'Edward', 'Ella', 'Oliver', 'Karen', 'Isabel', 'Brad']\n",
    "list_topics = ['fashion', 'politics', 'books', 'sports', 'general entertainment', 'music', 'science', 'technology', 'movies']\n",
    "\n",
    "for conv in range(11, 12):\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    filename = f'conv{conv}.html'\n",
    "    with open(filename, \"w\") as file_html:  # Use 'with' to properly manage file closing\n",
    "        \n",
    "        friend1 = np.random.choice(list_names1)\n",
    "        friend2 = np.random.choice(list_names2)\n",
    "        topic = np.random.choice(list_topics)\n",
    "        \n",
    "        #prompt = f\"{friend1} greets {friend2}. {friend2} starts to talk about {topic}. Both ask short questions, give short responses and often disagree. Then the topic changes. The conversation never ends.\\n{friend1}: Hi!\\n{friend2}:\"\n",
    "        #prompt = f\"{friend1} greets {friend2}. They discuss {topic}, asking engaging questions, and disagreeing often. Once they've shared their thoughts, they transition to a new topic smoothly and naturally, just like real friends. the conversation never ends. \\n\"+friend1+\": Hi! \\n\"+friend2+\":\"\n",
    "        #prompt = f\"{friend1} greets {friend2}. They discuss {topic}, but they don’t always agree. Each person has their own unique perspective, and they occasionally challenge each other. They avoid excessive agreement and mutual praise. They ensure to change topics when the discussion slows down.\"\n",
    "\n",
    "        personality1 = np.random.choice([\"logical and skeptical\", \"excitable and passionate\", \"sarcastic and witty\", \"serious and intellectual\"])\n",
    "        personality2 = np.random.choice([\"curious and open-minded\", \"pragmatic and reserved\", \"opinionated and strong-willed\", \"friendly and cheerful\"])\n",
    "\n",
    "        prompt = f\"{friend1} is {personality1} and is having a natural conversation with a friend. They discuss {topic}, but they don’t always agree. Each person has their own perspective, and they challenge each other at times. They avoid excessive agreement and mutual praise. They ensure to change topics when the discussion slows down.  Keep answers short, text-like, and very informal.**Do not ask a question at the end of your response. Conclude with a statement, reflection, or suggestion.**. \\n\" + \"A: Hi! \\n:\"\n",
    "        prompt2 = f\"{friend2} is {personality2} and is having a natural conversation with a friend. Avoid excessive agreement and looping. Keep answers under 50 tokens. Instead of repeating excitement, introduce new questions or challenge each other’s views. If a topic has been discussed for too long, transition to a new subject smoothly. Keep answers short, text-like, and very informal. DO NOT always end with questions.\"\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(12):\n",
    "            num_words = 0\n",
    "            cnt = 0\n",
    "            while num_words < 5 and cnt < 6:\n",
    "                temperature_step = temperature if cnt < 3 else temperature_regen\n",
    "                response_length = generate_response_length()\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"system\", \"content\": prompt2},\n",
    "                            {\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=min(60, int(response_length * 1.3)),  # Prevents abrupt cutoffs\n",
    "                    top_p=1.0,\n",
    "                    frequency_penalty=frequency_penalty,\n",
    "                    presence_penalty=presence_penalty,\n",
    "                    stop=[\"\\n\", f\"{friend1}:\", f\"{friend2}:\"]  # Stops at a natural break\n",
    "                \n",
    "                )\n",
    "\n",
    "                # Ensure response ends properly\n",
    "                string = response.choices[0].message.content.strip()\n",
    "                string = casual_text_formatting(string)\n",
    "                if not string.endswith((\".\", \"?\", \"!\", '\"')):  \n",
    "                    string += \"...\"\n",
    "                \n",
    "\n",
    "                num_words = len(string.split())\n",
    "                cnt += 1\n",
    "            \n",
    "            speaker = f\"\\n{friend1}:\"\n",
    "            new_line = f\"<p> A: {string} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "            prompt += string + speaker\n",
    "\n",
    "            num_words = 0\n",
    "            cnt = 0\n",
    "            while num_words < 5 and cnt < 6:\n",
    "                temperature_step = temperature if cnt < 3 else temperature_regen\n",
    "                response_length = generate_response_length()\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"system\", \"content\": prompt2},\n",
    "                            {\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=min(60, int(response_length * 1.3)),  # Prevents abrupt cutoffs\n",
    "                    top_p=1.0,\n",
    "                    frequency_penalty=frequency_penalty,\n",
    "                    presence_penalty=presence_penalty,\n",
    "                    stop=[\"\\n\", f\"{friend1}:\", f\"{friend2}:\"]  # Stops at a natural break\n",
    "                  \n",
    "                )\n",
    "\n",
    "                # Ensure response ends properly\n",
    "                string = response.choices[0].message.content.strip()\n",
    "                string = casual_text_formatting(string)\n",
    "                if not string.endswith((\".\", \"?\", \"!\", '\"')):  \n",
    "                    string += \"...\"\n",
    "\n",
    "                num_words = len(string.split())\n",
    "                cnt += 1\n",
    "            \n",
    "            speaker = f\"\\n{friend2}:\"\n",
    "            new_line = f\"<p> B: {string} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "            prompt += string + speaker\n",
    "\n",
    "        print(prompt)  # Print the final conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b59071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def casual_text_formatting(text):\n",
    "    \"\"\"Randomly convert responses to lowercase to mimic casual texting.\"\"\"\n",
    "    if random.random() < 0.5:  # 50% chance of being lowercase\n",
    "        return text.lower()\n",
    "    return text  # Otherwise, keep normal capitalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fde83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import openai\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Path to the folder containing HTML files\n",
    "html_folder = \"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/conversation_task/conversation_dataset\"\n",
    "\n",
    "# Function to extract word counts from <p> tags\n",
    "def extract_word_counts_from_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "        conversation_turns = soup.find_all(\"p\")\n",
    "        word_counts = [len(turn.get_text().split()) for turn in conversation_turns]\n",
    "        return word_counts\n",
    "\n",
    "# Aggregate word counts from specific HTML files\n",
    "all_word_counts = []\n",
    "for i in range(1, 41):\n",
    "    html_file = os.path.join(html_folder, f\"conv{i}.html\")\n",
    "    if os.path.exists(html_file):\n",
    "        all_word_counts.extend(extract_word_counts_from_html(html_file))\n",
    "\n",
    "# Function to generate AI-like response lengths\n",
    "def generate_response_length():\n",
    "    return np.random.choice(all_word_counts)\n",
    "\n",
    "# Function to truncate text at a natural stopping point\n",
    "def truncate_text(text, max_words):\n",
    "    words = text.split()\n",
    "    if len(words) <= max_words:\n",
    "        return text\n",
    "    truncated_text = \" \".join(words[:max_words])\n",
    "    return re.sub(r'[^.!?]*$', '', truncated_text).strip()  # Ensure it ends cleanly\n",
    "\n",
    "# OpenAI GPT conversation generation\n",
    "model = \"gpt-4o\" #\"gpt-4o\"\n",
    "temperature = 0.6\n",
    "frequency_penalty = 2\n",
    "presence_penalty = 2\n",
    "temperature_regen = 1\n",
    "\n",
    "list_names1 = ['John','Alice','Mark','Andrew','Julia', 'Georgia', 'Juliet', 'Lily', 'Olivia', 'Emmett', 'Miles', 'Oscar', 'William']\n",
    "list_names2 = ['Frank','Tom','Elizabeth','Gabriel','Paul', 'Alfie', 'Christina', 'Edward', 'Ella', 'Oliver', 'Karen', 'Isabel', 'Brad']\n",
    "list_topics = ['fashion', 'politics', 'books', 'sports', 'general entertainment', 'music', 'science', 'technology', 'movies']\n",
    "\n",
    "for conv in range(17, 18):\n",
    "    time.sleep(1)\n",
    "    filename = f'conv{conv}.html'\n",
    "    with open(filename, \"w\") as file_html:\n",
    "        friend1 = np.random.choice(list_names1)\n",
    "        friend2 = np.random.choice(list_names2)\n",
    "        topic = np.random.choice(list_topics)\n",
    "\n",
    "        personality1 = np.random.choice([\"logical and skeptical\", \"excitable and passionate\", \"sarcastic and witty\", \"serious and intellectual\"])\n",
    "        personality2 = np.random.choice([\"curious and open-minded\", \"pragmatic and reserved\", \"opinionated and strong-willed\", \"friendly and cheerful\"])\n",
    "\n",
    "        prompt = f\"{friend1} is {personality1} and is having a natural conversation with a friend. They discuss {topic}, but they don’t always agree. Each person has their own perspective, and they challenge each other at times. They avoid excessive agreement and mutual praise. Be spare with punctuation. You can insert one spelling mistake. They ensure to change topics when the discussion slows down.  Keep answers short, text-like, and very informal.**Do not ask a question at the end of your response. Conclude with a statement, reflection, or suggestion.**. \\n\" + \"A: Hi! \\n:\"\n",
    "        prompt2 = f\"{friend2} is {personality2} and is having a natural conversation with a friend. Avoid excessive agreement and looping. Keep answers under 20 tokens. Instead of repeating excitement, introduce new questions or challenge each other’s views. If a topic has been discussed for too long, transition to a new subject smoothly.  Be spare with punctuation. You can insert one spelling mistake. Keep answers short, text-like, and very informal. DO NOT always end with questions.\"\n",
    "\n",
    "\n",
    "        for i in range(12):\n",
    "            num_words = 0\n",
    "            cnt = 0\n",
    "            while num_words < 5 and cnt < 6:\n",
    "                response_length = generate_response_length()\n",
    "                \n",
    "                #if cnt<3:\n",
    "                #    temperature_step = temperature\n",
    "                #else:\n",
    "                #    temperature_step = temperature_regen\n",
    "                \n",
    "                temperature_step = temperature + ((temperature_regen - temperature) / 5) * cnt\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"system\", \"content\": prompt2},\n",
    "                              {\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=temperature_step,\n",
    "                    max_tokens=80,  # Keep strict token limit\n",
    "                    top_p=1.0,\n",
    "                    frequency_penalty=frequency_penalty,\n",
    "                    presence_penalty=presence_penalty,\n",
    "                    stop=[f\"{friend1}:\", f\"{friend2}:\"]\n",
    "                )\n",
    "                string = response.choices[0].message.content.strip()\n",
    "                string = truncate_text(string, response_length)  # Truncate cleanly\n",
    "                string = casual_text_formatting(string)\n",
    "                num_words = len(string.split())\n",
    "                cnt += 1\n",
    "            \n",
    "            speaker = f\"\\n{friend1}:\"\n",
    "            new_line = f\"<p> A: {string} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "            prompt += string + speaker\n",
    "\n",
    "            num_words = 0\n",
    "            cnt = 0\n",
    "            while num_words < 5 and cnt < 6:\n",
    "                response_length = generate_response_length()\n",
    "\n",
    "                #if cnt<3:\n",
    "                #    temperature_step = temperature\n",
    "                #else:\n",
    "                #    temperature_step = temperature_regen\n",
    "\n",
    "                temperature_step = temperature + ((temperature_regen - temperature) / 5) * cnt\n",
    "\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"system\", \"content\": prompt2},\n",
    "                              {\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=temperature_step,\n",
    "                    max_tokens=80,\n",
    "                    top_p=1.0,\n",
    "                    frequency_penalty=frequency_penalty,\n",
    "                    presence_penalty=presence_penalty,\n",
    "                    stop=[f\"{friend1}:\", f\"{friend2}:\"]\n",
    "                )\n",
    "                string = response.choices[0].message.content.strip()\n",
    "                string = truncate_text(string, response_length)\n",
    "                string = casual_text_formatting(string)\n",
    "                num_words = len(string.split())\n",
    "                cnt += 1\n",
    "            \n",
    "            speaker = f\"\\n{friend2}:\"\n",
    "            new_line = f\"<p> B: {string} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "            prompt += string + speaker\n",
    "        \n",
    "        print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e3c19",
   "metadata": {},
   "source": [
    "# Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"<DeepSeek API Key>\", base_url=\"https://api.deepseek.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ab049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# Round 1\n",
    "messages = [{\"role\": \"user\", \"content\": \"9.11 and 9.8, which is greater?\"}]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "# Round 2\n",
    "messages.append({'role': 'assistant', 'content': content})\n",
    "messages.append({'role': 'user', 'content': \"How many Rs are there in the word 'strawberry'?\"})\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=messages\n",
    ")\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "turing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
