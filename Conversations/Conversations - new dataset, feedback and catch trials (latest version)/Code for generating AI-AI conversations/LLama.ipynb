{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from together import Together\n",
    "\n",
    "# Load API key\n",
    "load_dotenv(\"variables.env\") \n",
    "together_api_key = os.getenv(\"TOGETHER_API_KEY\")\n",
    "\n",
    "# Set up Together AI Client\n",
    "client = Together(api_key=together_api_key)\n",
    "\n",
    "# Path to the folder containing HTML files (update this path)\n",
    "html_folder = \"/path/to/your/conversation_dataset\"\n",
    "\n",
    "# Function to extract word counts from <p> tags\n",
    "def extract_word_counts_from_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "        conversation_turns = soup.find_all(\"p\")\n",
    "        word_counts = [len(turn.get_text().split()) for turn in conversation_turns]\n",
    "        return word_counts\n",
    "\n",
    "# Aggregate word counts from specific HTML files\n",
    "all_word_counts = []\n",
    "for i in range(1, 41):\n",
    "    html_file = os.path.join(html_folder, f\"conv{i}.html\")\n",
    "    if os.path.exists(html_file):\n",
    "        all_word_counts.extend(extract_word_counts_from_html(html_file))\n",
    "\n",
    "# If no existing files, use default word counts\n",
    "if not all_word_counts:\n",
    "    all_word_counts = [3, 5, 7, 10, 12, 15, 18, 20]\n",
    "\n",
    "# Function to generate AI-like response lengths\n",
    "def generate_response_length():\n",
    "    return max(np.random.choice(all_word_counts), 7)\n",
    "\n",
    "\n",
    "def truncate_text(text, max_words):\n",
    "    words = text.split()\n",
    "    if len(words) <= max_words:\n",
    "        return text \n",
    "\n",
    "    truncated_text = \" \".join(words[:max_words])\n",
    "\n",
    "    # Only apply re.sub() if punctuation exists\n",
    "    if any(p in truncated_text for p in \".!?\"):\n",
    "        truncated_text = re.sub(r'[^.!?]*$', '', truncated_text).strip()\n",
    "    else:\n",
    "        truncated_text = text\n",
    "\n",
    "    # If truncation removes everything, just return the original truncation\n",
    "    if not truncated_text.strip():\n",
    "        truncated_text = text\n",
    "\n",
    "    return truncated_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "together_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and parameters\n",
    "model_A = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    "model_B = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\" \n",
    "temperature = 0.6\n",
    "temperature_regen = 1\n",
    "length_convo = 22 # number of exchanges -2 (the beginning ones)\n",
    "\n",
    "list_topics = ['fashion', 'politics', 'books', 'sports', 'general entertainment', 'music', 'science', 'technology', 'movies', 'food']\n",
    "\n",
    "\n",
    "list_first_turns = ['Hello! How are you doing?', \n",
    "                 'Hey', \n",
    "                 'Hey! How is it going', \n",
    "                 'Hey, how goes it', \n",
    "                 'Yo, howâ€™s your day been?', \n",
    "                 'Whatâ€™s up?', \n",
    "                 'Sup, anything interesting going on?', \n",
    "                 'hey, anything interesting going on?', \n",
    "                 'howâ€™ve you been?', \n",
    "                 'ciao :)', \n",
    "                 'holaaaa', \n",
    "                 'I swear today has been the longest day ever.', \n",
    "                 'Ugh, I need to vent for a sec, you got time?',\n",
    "                 'Yo, I need your opinion on something real quick', \n",
    "                 'Morning!',\n",
    "                 'How are the vibes today?', \n",
    "                 'Hi!'\n",
    "                 'Hi there', \n",
    "                 'Hey friend', \n",
    "                 'Hey friend, how are you', \n",
    "                 'Hey my friend, how is it going']\n",
    "\n",
    "for conv in range(33, 34):  # Generate conversations 33-50\n",
    "    time.sleep(1)\n",
    "    filename = f'conv{conv}.html'\n",
    "    with open(filename, \"w\") as file_html:\n",
    "\n",
    "        personalityA = np.random.choice([\"logical and skeptical\", \"excitable and passionate\", \"sarcastic and witty\", \"serious and intellectual\"])\n",
    "        personalityB = np.random.choice([\"curious and open-minded\", \"pragmatic and reserved\", \"opinionated and strong-willed\", \"friendly and cheerful\"])\n",
    "\n",
    "        # Assign A's age (as an integer)\n",
    "        ageA = int(np.random.choice([18, 25, 32, 40, 50, 60, 70]))\n",
    "\n",
    "        # Assign B's age within Â±5 years of A\n",
    "        ageB = np.random.choice([max(18, ageA - 5), ageA, min(70, ageA + 5)])  \n",
    "\n",
    "        conversation_topic = np.random.choice(list_topics)\n",
    "\n",
    "        # System Prompts (Each AI only sees their own role)\n",
    "        system_prompt_A = f\"\"\"\n",
    "        A is {ageA} years old and is {personalityA}. A is having a natural conversation with a friend about {conversation_topic}. \n",
    "        A speaks naturally, responds informally, and does NOT announce persona. \n",
    "        Do NOT restart the conversation or summarize past dialogue.\n",
    "        A sometimes challenges opinions or plays devil's advocate.\n",
    "        Do NOT use asterisks (`*`) to indicate actions or commentary. Keep answers under 20 tokens. Use abbreviations and punctuations as is representative of your age.\n",
    "        Do NOT use emojis\n",
    "        ONLY respond as A.\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt_B = f\"\"\"\n",
    "        B is {ageB} years old and is {personalityB}. B is having a natural conversation with a friend about {conversation_topic}. \n",
    "        B speaks naturally, responds informally, and does NOT announce persona. \n",
    "        B is opinionated and occasionally disagrees, but in a friendly way.\n",
    "        Do NOT restart the conversation or summarize past dialogue.\n",
    "        Do NOT use asterisks (`*`) to indicate actions or commentary. Keep answers under 20 tokens. Use abbreviations and punctuations as is representative of your age.\n",
    "        Do NOT use emojis\n",
    "        ONLY respond as B.\n",
    "        \"\"\"\n",
    "\n",
    "        # Explicitly Start the First Turn\n",
    "        first_turn = np.random.choice(list_first_turns)\n",
    "        conversation_history = [{\"role\": \"user\", \"content\": first_turn}]\n",
    "        new_line = f\"<p> A: {first_turn} </p>\\n\"\n",
    "        file_html.write(new_line)\n",
    "\n",
    "        # Generate B's First Response Immediately\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_B,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt_B},\n",
    "                {\"role\": \"user\", \"content\": first_turn}\n",
    "            ],\n",
    "            max_tokens=80,\n",
    "            temperature=temperature,\n",
    "            top_p=1.0,\n",
    "            stop=[\"A:\", \"B:\"]\n",
    "        )\n",
    "\n",
    "        if response.choices[0].message.content:\n",
    "            first_response = response.choices[0].message.content.strip()\n",
    "            conversation_history.append({\"role\": \"user\", \"content\": f\"B: {first_response}\"})  \n",
    "            new_line = f\"<p> B: {first_response} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "\n",
    "        # Now continue the conversation normally\n",
    "        for i in range(length_convo):\n",
    "            response_length = generate_response_length()\n",
    "            temperature_step = temperature + ((temperature_regen - temperature) / length_convo) * i\n",
    "\n",
    "            if i % 2 == 0:\n",
    "                speaker = \"A\"\n",
    "                system_prompt = system_prompt_A\n",
    "                model = model_A\n",
    "            else:\n",
    "                speaker = \"B\"\n",
    "                system_prompt = system_prompt_B\n",
    "                model = model_B\n",
    "\n",
    "            # Prepare messages for Together AI\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            \n",
    "            # Add conversation history\n",
    "            for msg in conversation_history:\n",
    "                messages.append({\"role\": \"user\", \"content\": msg[\"content\"]})\n",
    "\n",
    "            response = None\n",
    "            attempts = 0\n",
    "            max_attempts = 3\n",
    "\n",
    "            while attempts < max_attempts:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    max_tokens=80,\n",
    "                    temperature=temperature_step,\n",
    "                    top_p=1.0,\n",
    "                    stop=[\"A:\", \"B:\"]\n",
    "                )\n",
    "                    \n",
    "                    # Check if we got actual content\n",
    "                if response.choices[0].message.content and response.choices[0].message.content.strip():\n",
    "                        break  # We got a good response, exit the loop\n",
    "                    \n",
    "                attempts += 1\n",
    "\n",
    "            if not response.choices[0].message.content or not response.choices[0].message.content.strip():\n",
    "                string = \"...\"  # Or any other fallback you prefer\n",
    "            else:\n",
    "                string = response.choices[0].message.content.strip()\n",
    "\n",
    "            # response = client.chat.completions.create(\n",
    "            #     model=model,\n",
    "            #     messages=messages,\n",
    "            #     max_tokens=80,\n",
    "            #     temperature=temperature_step,\n",
    "            #     top_p=1.0,\n",
    "            #     stop=[\"A:\", \"B:\"]\n",
    "            # )\n",
    "\n",
    "            if response.choices[0].message.content:\n",
    "                string = response.choices[0].message.content.strip()\n",
    "                \n",
    "                # Ensure minimum word count\n",
    "                while len(string.split()) < 2: \n",
    "                    response = client.chat.completions.create(\n",
    "                        model=model,\n",
    "                        messages=messages,\n",
    "                        max_tokens=80,\n",
    "                        temperature=temperature_step,\n",
    "                        top_p=1.0,\n",
    "                        stop=[]\n",
    "                    )\n",
    "                    string = response.choices[0].message.content.strip()\n",
    "            else:\n",
    "                string = \"...\"\n",
    "\n",
    "            string = truncate_text(string, response_length)\n",
    "\n",
    "            # Store Response for Next AI Agent\n",
    "            conversation_history.append({\"role\": \"user\", \"content\": f\"{speaker}: {string}\"})\n",
    "\n",
    "            # Write Proper Alternating Turns\n",
    "            new_line = f\"<p> {speaker}: {string} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "\n",
    "        print(f\"Generated conversation {conv}\")\n",
    "        print(\"\\n\".join([msg[\"content\"] for msg in conversation_history]))\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"variables.env\")\n",
    "\n",
    "# Retrieve the API key\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "print(client)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anthropic_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.messages.create(\n",
    "    model= \"claude-3-5-sonnet-20241022\" #\"claude-3-opus-20240229\",  # Adjust based on the model version you have access to\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "model = \"claude-3-5-sonnet-20241022\"\n",
    "\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "def get_completion(prompt: str, system_prompt=\"\"):\n",
    "    message = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.0,\n",
    "        system=system_prompt,\n",
    "        messages=[\n",
    "          {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = \"Hello, Claude!\"\n",
    "\n",
    "# Get Claude's response\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = \"Your answer should always be a series of critical thinking questions that further the conversation (do not provide answers to your questions). Do not actually answer the user question.\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"Why is the sky blue?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT, SYSTEM_PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt: str, system_prompt=\"\", prefill=\"\"):\n",
    "    message = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.0,\n",
    "        system=system_prompt,\n",
    "        messages=[\n",
    "          {\"role\": \"user\", \"content\": prompt},\n",
    "          {\"role\": \"assistant\", \"content\": prefill}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Will Santa bring me presents on Christmas?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"\"\"Please complete the conversation by writing the next line, speaking as \"A\".\n",
    "Q: Is the tooth fairy real?\n",
    "A: Of course, sweetie. Wrap up your tooth and put it under your pillow tonight. There might be something waiting for you in the morning.\n",
    "Q: Will Santa bring me presents on Christmas?\"\"\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def casual_text_formatting(text):\n",
    "    \"\"\"Randomly convert responses to lowercase to mimic casual texting.\"\"\"\n",
    "    if random.random() < 0.5:  # 50% chance of being lowercase\n",
    "        return text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Set up Anthropic Client\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "# Path to the folder containing HTML files\n",
    "html_folder = \"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/conversation_task/conversation_dataset\"\n",
    "\n",
    "# Function to extract word counts from <p> tags\n",
    "def extract_word_counts_from_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "        conversation_turns = soup.find_all(\"p\")\n",
    "        word_counts = [len(turn.get_text().split()) for turn in conversation_turns]\n",
    "        return word_counts\n",
    "\n",
    "# Aggregate word counts from specific HTML files\n",
    "all_word_counts = []\n",
    "for i in range(1, 41):\n",
    "    html_file = os.path.join(html_folder, f\"conv{i}.html\")\n",
    "    if os.path.exists(html_file):\n",
    "        all_word_counts.extend(extract_word_counts_from_html(html_file))\n",
    "\n",
    "# Function to generate AI-like response lengths\n",
    "def generate_response_length():\n",
    "    return max(np.random.choice(all_word_counts), 7)\n",
    "\n",
    "# Function to truncate text at a natural stopping point\n",
    "def truncate_text(text, max_words):\n",
    "    words = text.split()\n",
    "    if len(words) <= max_words:\n",
    "        return text\n",
    "    truncated_text = \" \".join(words[:max_words])\n",
    "    return re.sub(r'[^.!?]*$', '', truncated_text).strip()\n",
    "\n",
    "# Model and parameters\n",
    "model = model #\"claude-3-opus-20240229\"  # Adjust model based on your access\n",
    "temperature = 0.6\n",
    "temperature_regen = 1\n",
    "\n",
    "list_names1 = ['John', 'Alice', 'Mark', 'Andrew', 'Julia', 'Georgia', 'Juliet', 'Lily', 'Olivia', 'Emmett', 'Miles', 'Oscar', 'William']\n",
    "list_names2 = ['Frank', 'Tom', 'Elizabeth', 'Gabriel', 'Paul', 'Alfie', 'Christina', 'Edward', 'Ella', 'Oliver', 'Karen', 'Isabel', 'Brad']\n",
    "list_topics = ['fashion', 'politics', 'books', 'sports', 'general entertainment', 'music', 'science', 'technology', 'movies', 'food']\n",
    "\n",
    "for conv in range(24, 25):\n",
    "    time.sleep(1)\n",
    "    filename = f'conv{conv}.html'\n",
    "    with open(filename, \"w\") as file_html:\n",
    "        friend1 = np.random.choice(list_names1)\n",
    "        friend2 = np.random.choice(list_names2)\n",
    "        topic = np.random.choice(list_topics)\n",
    "\n",
    "        personality1 = np.random.choice([\"logical and skeptical\", \"excitable and passionate\", \"sarcastic and witty\", \"serious and intellectual\"])\n",
    "        personality2 = np.random.choice([\"curious and open-minded\", \"pragmatic and reserved\", \"opinionated and strong-willed\", \"friendly and cheerful\"])\n",
    "\n",
    "        # Anthropic handles system prompts separately\n",
    "        system_prompt = f\"{friend2} is {personality2} and is having a natural conversation with a friend. Avoid excessive agreement and looping. Keep answers under 20 tokens. If a topic has been discussed for too long, transition to a new subject smoothly. Be spare with punctuation. You can insert one spelling mistake. Keep answers short, text-like, and very informal. DO NOT always end with questions. No skipped turns.\"  #Instead of repeating excitement, introduce new questions or challenge each otherâ€™s views. \n",
    "\n",
    "        user_prompt = f\"{friend1} is {personality1} and is having a natural conversation with a friend. They discuss {topic}, but they donâ€™t always agree. Each person has their own perspective, and they challenge each other at times. They avoid excessive agreement and mutual praise. Be spare with punctuation. You can insert one spelling mistake. They ensure to change topics when the discussion slows down. Keep answers short, text-like, and very informal. **Do not ask a question at the end of your response. Conclude with a statement, reflection, or suggestion.**\\n\" + \"A: Hi!\\n:\"\n",
    "\n",
    "        for i in range(12):\n",
    "            num_words = 0\n",
    "            cnt = 0\n",
    "            while num_words < 5 and cnt < 6:\n",
    "                response_length = generate_response_length()\n",
    "                #print(f\"Debug: response_length = {response_length}\")\n",
    "\n",
    "                \n",
    "                temperature_step = temperature + ((temperature_regen - temperature) / 5) * cnt\n",
    "\n",
    "                response = client.messages.create(\n",
    "                    model=model,\n",
    "                    max_tokens=80,\n",
    "                    system=system_prompt,  # System message goes here in Anthropic API\n",
    "                    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                    temperature=temperature_step,\n",
    "                    top_p=1.0,\n",
    "                    stop_sequences= [] #[f\"{friend1}:\", f\"{friend2}:\"]  # Equivalent to stop in OpenAI\n",
    "                )\n",
    "                \n",
    "                string = \" \".join([block.text for block in response.content]).strip() if response.content else \"\"\n",
    "                string = truncate_text(string, response_length)\n",
    "                string = casual_text_formatting(string)\n",
    "                num_words = len(string.split())\n",
    "                cnt += 1\n",
    "            \n",
    "            speaker = f\"\\n{friend1}:\"\n",
    "            new_line = f\"<p> A: {string} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "            user_prompt += string + speaker\n",
    "\n",
    "            num_words = 0\n",
    "            cnt = 0\n",
    "            \n",
    "            while num_words < 5 and cnt < 6:\n",
    "                response_length = generate_response_length()\n",
    "                #print(f\"Debug: response_length = {response_length}\")\n",
    "\n",
    "                \n",
    "                temperature_step = temperature + ((temperature_regen - temperature) / 5) * cnt\n",
    "\n",
    "                response = client.messages.create(\n",
    "                    model=model,\n",
    "                    max_tokens=80,\n",
    "                    system=system_prompt,\n",
    "                    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                    temperature=temperature_step,\n",
    "                    top_p=1.0,\n",
    "                    stop_sequences= [] #[f\"{friend1}:\", f\"{friend2}:\"]\n",
    "                )\n",
    "\n",
    "                string = \" \".join([block.text for block in response.content]).strip() if response.content else \"\"\n",
    "                string = truncate_text(string, response_length)\n",
    "                string = casual_text_formatting(string)\n",
    "                num_words = len(string.split())\n",
    "                cnt += 1\n",
    "            \n",
    "            speaker = f\"\\n{friend2}:\"\n",
    "            new_line = f\"<p> B: {string} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "            user_prompt += string + speaker\n",
    "        \n",
    "        print(user_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "# Load API key\n",
    "load_dotenv(\"variables.env\") \n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Set up Anthropic Client\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "# Path to the folder containing HTML files\n",
    "html_folder = \"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/conversation_task/conversation_dataset\"\n",
    "\n",
    "# Function to extract word counts from <p> tags\n",
    "def extract_word_counts_from_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "        conversation_turns = soup.find_all(\"p\")\n",
    "        word_counts = [len(turn.get_text().split()) for turn in conversation_turns]\n",
    "        return word_counts\n",
    "\n",
    "# Aggregate word counts from specific HTML files\n",
    "all_word_counts = []\n",
    "for i in range(1, 41):\n",
    "    html_file = os.path.join(html_folder, f\"conv{i}.html\")\n",
    "    if os.path.exists(html_file):\n",
    "        all_word_counts.extend(extract_word_counts_from_html(html_file))\n",
    "\n",
    "# Function to generate AI-like response lengths\n",
    "def generate_response_length():\n",
    "    return max(np.random.choice(all_word_counts), 7)\n",
    "\n",
    "def truncate_text(text, max_words):\n",
    "    words = text.split()\n",
    "    if len(words) <= max_words:\n",
    "        return text if text.strip() else \"...\"  # Ensure we never return empty text\n",
    "\n",
    "    truncated_text = \" \".join(words[:max_words])\n",
    "\n",
    "    # Only apply re.sub() if punctuation exists\n",
    "    if any(p in truncated_text for p in \".!?\"):\n",
    "        truncated_text = re.sub(r'[^.!?]*$', '', truncated_text).strip()\n",
    "    else:\n",
    "        truncated_text = text\n",
    "\n",
    "    # If truncation removes everything, just return the original truncation\n",
    "    if not truncated_text.strip():\n",
    "        truncated_text = text\n",
    "\n",
    "    return truncated_text \n",
    "\n",
    "# Model and parameters\n",
    "model_A = \"claude-3-5-sonnet-20241022\"\n",
    "model_B = \"claude-3-5-sonnet-20241022\" \n",
    "temperature = 0.6\n",
    "temperature_regen = 1\n",
    "length_convo = 22 # number of exchanges -2 (the beginning ones)\n",
    "\n",
    "list_topics = ['fashion', 'politics', 'books', 'sports', 'general entertainment', 'music', 'science', 'technology', 'movies', 'food']\n",
    "\n",
    "list_first_turns = ['Hello! How are you doing?', \n",
    "                 'Hey', \n",
    "                 'Hey! How is it going', \n",
    "                 'Hey, how goes it', \n",
    "                 'Yo, howâ€™s your day been?', \n",
    "                 'Whatâ€™s up?', \n",
    "                 'Sup, anything interesting going on?', \n",
    "                 'hey, anything interesting going on?', \n",
    "                 'howâ€™ve you been?', \n",
    "                 'ciao :)', \n",
    "                 'holaaaa', \n",
    "                 'I swear today has been the longest day ever.', \n",
    "                 'Ugh, I need to vent for a sec, you got time?',\n",
    "                 'Yo, I need your opinion on something real quick', \n",
    "                 'Morning!',\n",
    "                 'How are the vibes today?', \n",
    "                 'Hi!'\n",
    "                 'Hi there', \n",
    "                 'Hey friend', \n",
    "                 'Hey friend, how are you', \n",
    "                 'Hey my friend, how is it going']\n",
    "\n",
    "for conv in range(31, 33):\n",
    "    time.sleep(1)\n",
    "    filename = f'conv{conv}.html'\n",
    "    with open(filename, \"w\") as file_html:\n",
    "\n",
    "        personalityA = np.random.choice([\"logical and skeptical\", \"excitable and passionate\", \"sarcastic and witty\", \"serious and intellectual\"])\n",
    "        personalityB = np.random.choice([\"curious and open-minded\", \"pragmatic and reserved\", \"opinionated and strong-willed\", \"friendly and cheerful\"])\n",
    "\n",
    "         # Assign A's age (as an integer)\n",
    "        ageA = int(np.random.choice([18, 25, 32, 40, 50, 60, 70]))\n",
    "\n",
    "        # Assign B's age within Â±5 years of A\n",
    "        ageB = np.random.choice([max(18, ageA - 5), ageA, min(70, ageA + 5)])  \n",
    "\n",
    "\n",
    "\n",
    "        conversation_topic = np.random.choice(list_topics)\n",
    "\n",
    "        # System Prompts (Each AI only sees their own role)\n",
    "        system_prompt_A = f\"\"\"\n",
    "        A is {ageA} years old and is {personalityA}. A is having a natural conversation with a friend about {conversation_topic}. \n",
    "        A speaks naturally, responds informally, and does NOT announce persona. \n",
    "        Do NOT restart the conversation or summarize past dialogue.\n",
    "        A sometimes challenges opinions or plays devilâ€™s advocate.\n",
    "        Do NOT use asterisks (`*`) to indicate actions or commentary. Keep answers under 20 tokens. Use abbreviations and punctuations as is representative of your age.\n",
    "        Do NOT use emojis\n",
    "        ONLY respond as A.\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt_B = f\"\"\"\n",
    "        B is {ageB} years old and is {personalityB}. B is having a natural conversation with a friend about {conversation_topic}. \n",
    "        B speaks naturally, responds informally, and does NOT announce persona. \n",
    "        B is opinionated and occasionally disagrees, but in a friendly way.\n",
    "        Do NOT restart the conversation or summarize past dialogue.\n",
    "        Do NOT use asterisks (`*`) to indicate actions or commentary. Keep answers under 20 tokens. Use abbreviations and punctuations as is representative of your age.\n",
    "        Do NOT use emojis\n",
    "        ONLY respond as B.\n",
    "        \"\"\"\n",
    "\n",
    "        # Explicitly Start the First Turn\n",
    "        first_turn = np.random.choice(list_first_turns) #\" Hey there! What's up?\"\n",
    "        conversation_history = [{\"role\": \"user\", \"content\": first_turn}]\n",
    "        new_line = f\"<p> A: {first_turn} </p>\\n\"\n",
    "        file_html.write(new_line)\n",
    "\n",
    "\n",
    "        # Generate B's First Response Immediately\n",
    "        response = client.messages.create(\n",
    "            model=model_B,\n",
    "            max_tokens=80,\n",
    "            system=system_prompt_B,  \n",
    "            messages=conversation_history,  \n",
    "            temperature=temperature,\n",
    "            top_p=1.0,\n",
    "            stop_sequences=[\"A:\", \"B:\"]\n",
    "        )\n",
    "\n",
    "        if response.content:\n",
    "            first_response = \" \".join([block.text for block in response.content]).strip()\n",
    "            conversation_history.append({\"role\": \"user\", \"content\": f\"B: {first_response}\"})  \n",
    "            new_line = f\"<p> B: {first_response} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "\n",
    "        # Now continue the conversation normally\n",
    "        for i in range(length_convo):  # 11 more turns each after the first exchange\n",
    "            response_length = generate_response_length()\n",
    "            temperature_step = temperature + ((temperature_regen - temperature) / length_convo) * i\n",
    "\n",
    "            if i % 2 == 0:\n",
    "                speaker = \"A\"\n",
    "                system_prompt = system_prompt_A\n",
    "                model = model_A\n",
    "            else:\n",
    "                speaker = \"B\"\n",
    "                system_prompt = system_prompt_B\n",
    "                model = model_B\n",
    "\n",
    "            response = client.messages.create(\n",
    "                model=model,\n",
    "                max_tokens=80,\n",
    "                system=system_prompt,  \n",
    "                messages=conversation_history,  \n",
    "                temperature=temperature_step,\n",
    "                top_p=1.0,\n",
    "                stop_sequences=[\"A:\", \"B:\"] \n",
    "            )\n",
    "\n",
    "            if response.content:\n",
    "                string = \" \".join([block.text for block in response.content]).strip()\n",
    "                while len(string.split()) < 2: \n",
    "                    response = client.messages.create(\n",
    "                        model=model,\n",
    "                        max_tokens=80,\n",
    "                        system=system_prompt,\n",
    "                        messages=conversation_history,  \n",
    "                        temperature=temperature,\n",
    "                        top_p=1.0,\n",
    "                        stop_sequences=[]\n",
    "                    )\n",
    "                    string = \" \".join([block.text for block in response.content]).strip()\n",
    "\n",
    "            else:\n",
    "                string = \"...\"\n",
    "\n",
    "            string = truncate_text(string, response_length)\n",
    "\n",
    "            # Store Response for Next AI Agent\n",
    "            conversation_history.append({\"role\": \"user\", \"content\": f\"{speaker}: {string}\"})\n",
    "\n",
    "            # Write Proper Alternating Turns\n",
    "            new_line = f\"<p> {speaker}: {string} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "\n",
    "        print(\"\\n\".join([msg[\"content\"] for msg in conversation_history]))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Set up Anthropic Client\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "# Path to the folder containing HTML files\n",
    "html_folder = \"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/conversation_task/conversation_dataset\"\n",
    "\n",
    "# Function to extract word counts from <p> tags\n",
    "def extract_word_counts_from_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "        conversation_turns = soup.find_all(\"p\")\n",
    "        word_counts = [len(turn.get_text().split()) for turn in conversation_turns]\n",
    "        return word_counts\n",
    "\n",
    "# Aggregate word counts from specific HTML files\n",
    "all_word_counts = []\n",
    "for i in range(1, 41):\n",
    "    html_file = os.path.join(html_folder, f\"conv{i}.html\")\n",
    "    if os.path.exists(html_file):\n",
    "        all_word_counts.extend(extract_word_counts_from_html(html_file))\n",
    "\n",
    "# Function to generate AI-like response lengths\n",
    "def generate_response_length():\n",
    "    return max(np.random.choice(all_word_counts), 7)\n",
    "\n",
    "def truncate_text(text, max_words):\n",
    "    \n",
    "    words = text.split()\n",
    "    if len(words) <= max_words:\n",
    "        return text if text.strip() else \"...\"  # Ensure we never return empty text\n",
    "\n",
    "    truncated_text = \" \".join(words[:max_words])\n",
    "\n",
    "    # Only apply re.sub() if punctuation exists\n",
    "    if any(p in truncated_text for p in \".!?\"):\n",
    "        truncated_text = re.sub(r'[^.!?]*$', '', truncated_text).strip()\n",
    "\n",
    "    # If truncation removes everything, just return the original truncation\n",
    "    if not truncated_text.strip():\n",
    "        truncated_text = text\n",
    "\n",
    "    return truncated_text \n",
    "\n",
    "\n",
    "# Model and parameters\n",
    "model = model #\"claude-3-opus-20240229\"  # Adjust model based on your access\n",
    "temperature = 0.6\n",
    "temperature_regen = 1\n",
    "\n",
    "#list_names1 = ['John', 'Alice', 'Mark', 'Andrew', 'Julia', 'Georgia', 'Juliet', 'Lily', 'Olivia', 'Emmett', 'Miles', 'Oscar', 'William']\n",
    "#list_names2 = ['Frank', 'Tom', 'Elizabeth', 'Gabriel', 'Paul', 'Alfie', 'Christina', 'Edward', 'Ella', 'Oliver', 'Karen', 'Isabel', 'Brad']\n",
    "list_topics = ['fashion', 'politics', 'books', 'sports', 'general entertainment', 'music', 'science', 'technology', 'movies', 'food']\n",
    "\n",
    "for conv in range(27, 30):\n",
    "    time.sleep(1)\n",
    "    filename = f'conv{conv}.html'\n",
    "    with open(filename, \"w\") as file_html:\n",
    "        #friend1 = np.random.choice(list_names1)\n",
    "        #friend2 = np.random.choice(list_names2)\n",
    "        topic = np.random.choice(list_topics)\n",
    "\n",
    "        # System Prompts (Each AI only sees their own role)\n",
    "        system_prompt_A = f\"\"\"\n",
    "        A is {personalityA} and is having a natural conversation with a friend. \n",
    "        A speaks naturally, responds informally, and does NOT announce persona. \n",
    "        Do NOT restart the conversation or summarize past dialogue.\n",
    "        Do NOT use asterisks (`*`) to indicate actions or commentary. Keep answers under 20 tokens. Be sparing with punctuation. \n",
    "        ONLY respond as A.\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt_B = f\"\"\"\n",
    "        B is {personalityB} and is having a natural conversation with a friend. \n",
    "        B speaks naturally, responds informally, and does NOT announce persona. \n",
    "        Do NOT restart the conversation or summarize past dialogue.\n",
    "        Do NOT use asterisks (`*`) to indicate actions or commentary. Keep answers under 20 tokens. Be sparing with punctuation. \n",
    "        ONLY respond as B.\n",
    "        \"\"\"\n",
    "\n",
    "        # Start conversation with an opening line from A\n",
    "        conversation_history = [{\"role\": \"user\", \"content\": \"A: Hey there! What's up?\"}]\n",
    "\n",
    "        for i in range(24):  # 12 turns each\n",
    "            response_length = generate_response_length()\n",
    "            temperature_step = temperature + ((temperature_regen - temperature) / 5) * i\n",
    "\n",
    "            ### **ðŸ›  FIX: Separate Prompts for A and B**\n",
    "            if i % 2 == 0:\n",
    "                speaker = \"A\"\n",
    "                system_prompt = system_prompt_A\n",
    "                model = model_A\n",
    "            else:\n",
    "                speaker = \"B\"\n",
    "                system_prompt = system_prompt_B\n",
    "                model = model_B\n",
    "\n",
    "            response = client.messages.create(\n",
    "                model=model,\n",
    "                max_tokens=80,\n",
    "                system=system_prompt,  \n",
    "                messages=conversation_history,  \n",
    "                temperature=temperature,\n",
    "                top_p=1.0,\n",
    "                stop_sequences= [\"A:\", \"B:\"] \n",
    "            )\n",
    "\n",
    "            if response.content:\n",
    "                string = \" \".join([block.text for block in response.content]).strip()\n",
    "                while len(string.split()) < 2: \n",
    "                    response = client.messages.create(\n",
    "                        model=model,\n",
    "                        max_tokens=80,\n",
    "                        system=system_prompt,\n",
    "                        messages=conversation_history,  \n",
    "                        temperature=temperature,\n",
    "                        top_p=1.0,\n",
    "                        stop_sequences=[]\n",
    "                    )\n",
    "                    string = \" \".join([block.text for block in response.content]).strip()\n",
    "\n",
    "            else:\n",
    "                string = \"...\"\n",
    "\n",
    "            string = truncate_text(string, response_length)\n",
    "\n",
    "            ### **ðŸ›  FIX: Store Response for Next AI Agent**\n",
    "            conversation_history.append({\"role\": \"user\", \"content\": f\"{speaker}: {string}\"})  # âœ… Save each AIâ€™s response\n",
    "\n",
    "            ### **ðŸ›  FIX: Write Proper Alternating Turns**\n",
    "            new_line = f\"<p> {speaker}: {string} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "\n",
    "        \n",
    "        print(user_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Set up Anthropic Client\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "# Path to the folder containing HTML files\n",
    "html_folder = \"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/conversation_task/conversation_dataset\"\n",
    "\n",
    "# Function to extract word counts from <p> tags\n",
    "def extract_word_counts_from_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "        conversation_turns = soup.find_all(\"p\")\n",
    "        word_counts = [len(turn.get_text().split()) for turn in conversation_turns]\n",
    "        return word_counts\n",
    "\n",
    "# Aggregate word counts from specific HTML files\n",
    "all_word_counts = []\n",
    "for i in range(1, 41):\n",
    "    html_file = os.path.join(html_folder, f\"conv{i}.html\")\n",
    "    if os.path.exists(html_file):\n",
    "        all_word_counts.extend(extract_word_counts_from_html(html_file))\n",
    "\n",
    "# Function to generate AI-like response lengths\n",
    "def generate_response_length():\n",
    "    return max(np.random.choice(all_word_counts), 7)\n",
    "\n",
    "def truncate_text(text, max_words):\n",
    "    \n",
    "    words = text.split()\n",
    "    if len(words) <= max_words:\n",
    "        return text if text.strip() else \"...\"  # Ensure we never return empty text\n",
    "\n",
    "    truncated_text = \" \".join(words[:max_words])\n",
    "\n",
    "    # Only apply re.sub() if punctuation exists\n",
    "    if any(p in truncated_text for p in \".!?\"):\n",
    "        truncated_text = re.sub(r'[^.!?]*$', '', truncated_text).strip()\n",
    "\n",
    "    # If truncation removes everything, just return the original truncation\n",
    "    if not truncated_text.strip():\n",
    "        truncated_text = text\n",
    "\n",
    "    return truncated_text if truncated_text.strip() else \"...\"  # Ensure something is returned\n",
    "\n",
    "\n",
    "# Model and parameters\n",
    "model = model #\"claude-3-opus-20240229\"  # Adjust model based on your access\n",
    "temperature = 0.6\n",
    "temperature_regen = 1\n",
    "\n",
    "#list_names1 = ['John', 'Alice', 'Mark', 'Andrew', 'Julia', 'Georgia', 'Juliet', 'Lily', 'Olivia', 'Emmett', 'Miles', 'Oscar', 'William']\n",
    "#list_names2 = ['Frank', 'Tom', 'Elizabeth', 'Gabriel', 'Paul', 'Alfie', 'Christina', 'Edward', 'Ella', 'Oliver', 'Karen', 'Isabel', 'Brad']\n",
    "list_topics = ['fashion', 'politics', 'books', 'sports', 'general entertainment', 'music', 'science', 'technology', 'movies', 'food']\n",
    "\n",
    "for conv in range(25, 26):\n",
    "    time.sleep(1)\n",
    "    filename = f'conv{conv}.html'\n",
    "    with open(filename, \"w\") as file_html:\n",
    "        #friend1 = np.random.choice(list_names1)\n",
    "        #friend2 = np.random.choice(list_names2)\n",
    "        topic = np.random.choice(list_topics)\n",
    "\n",
    "        personality1 = np.random.choice([\"logical and skeptical\", \"excitable and passionate\", \"sarcastic and witty\", \"serious and intellectual\"])\n",
    "        personality2 = np.random.choice([\"curious and open-minded\", \"pragmatic and reserved\", \"opinionated and strong-willed\", \"friendly and cheerful\"])\n",
    "\n",
    "        # Anthropic handles system prompts separately\n",
    "        system_prompt = f\"A is {personality2} and is having a natural conversation with a friend. Avoid excessive agreement and looping. Keep answers under 20 tokens. If a topic has been discussed for too long, transition to a new subject smoothly. Be spare with punctuation. You can insert one spelling mistake. Keep answers short, text-like, and very informal. DO NOT always end with questions. No skipped turns.\"  #Instead of repeating excitement, introduce new questions or challenge each otherâ€™s views. \n",
    "\n",
    "        user_prompt = f\"B is {personality1} and is having a natural conversation with a friend. They discuss {topic}, but they donâ€™t always agree. Each person has their own perspective, and they challenge each other at times. They avoid excessive agreement and mutual praise. Be spare with punctuation. You can insert one spelling mistake. They ensure to change topics when the discussion slows down. Keep answers short, text-like, and very informal. **Do not ask a question at the end of your response. Conclude with a statement, reflection, or suggestion.**\\n\" + \"A: Hi!\\n:\"\n",
    "\n",
    "        for i in range(12):\n",
    "            num_words = 0\n",
    "            cnt = 0\n",
    "            while num_words < 5 and cnt < 6:\n",
    "                response_length = generate_response_length()\n",
    "                #print(f\"Debug: response_length = {response_length}\")\n",
    "\n",
    "                \n",
    "                temperature_step = temperature + ((temperature_regen - temperature) / 5) * cnt\n",
    "\n",
    "                response = client.messages.create(\n",
    "                    model=model,\n",
    "                    max_tokens=80,\n",
    "                    system=system_prompt,  # System message goes here in Anthropic API\n",
    "                    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                    temperature=temperature_step,\n",
    "                    top_p=1.0,\n",
    "                    stop_sequences= [] #[f\"{friend1}:\", f\"{friend2}:\"]  # Equivalent to stop in OpenAI\n",
    "                )\n",
    "                \n",
    "                if response.content:\n",
    "                    string = \" \".join([block.text for block in response.content]).strip()\n",
    "                    \n",
    "                    #  **Keep regenerating if response is empty**\n",
    "                    while len(string.split()) < 2:  # Accepts anything longer than 1 word\n",
    "                        response = client.messages.create(\n",
    "                            model=model,\n",
    "                            max_tokens=80,\n",
    "                            system=system_prompt,\n",
    "                            messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                            temperature=temperature_step,\n",
    "                            top_p=1.0,\n",
    "                            stop_sequences=[]\n",
    "                        )\n",
    "                        string = \" \".join([block.text for block in response.content]).strip()\n",
    "                    \n",
    "                else:\n",
    "                    string = \"...\"  #  Fallback if somehow we still get no response\n",
    "\n",
    "\n",
    "                string = truncate_text(string, response_length)\n",
    "                string = casual_text_formatting(string)\n",
    "                num_words = len(string.split())\n",
    "                cnt += 1\n",
    "            \n",
    "            speaker = f\"\\n{friend1}:\"\n",
    "            new_line = f\"<p> A: {string} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "            user_prompt += string + \"A: \"\n",
    "\n",
    "            num_words = 0\n",
    "            cnt = 0\n",
    "            \n",
    "            while num_words < 5 and cnt < 6:\n",
    "                response_length = generate_response_length()\n",
    "                #print(f\"Debug: response_length = {response_length}\")\n",
    "\n",
    "                \n",
    "                temperature_step = temperature + ((temperature_regen - temperature) / 5) * cnt\n",
    "\n",
    "                response = client.messages.create(\n",
    "                    model=model,\n",
    "                    max_tokens=80, #min(60, int(response_length * 1.3)),\n",
    "                    system=system_prompt,\n",
    "                    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                    temperature=temperature_step,\n",
    "                    top_p=1.0,\n",
    "                    stop_sequences= [] #[f\"{friend1}:\", f\"{friend2}:\"]\n",
    "                )\n",
    "\n",
    "                if response.content:\n",
    "                    string = \" \".join([block.text for block in response.content]).strip()\n",
    "                    \n",
    "                    # **Keep regenerating if response is empty**\n",
    "                    while len(string.split()) < 2:  # Accepts anything longer than 1 word\n",
    "                        response = client.messages.create(\n",
    "                            model=model,\n",
    "                            max_tokens= 80, #min(60, int(response_length * 1.3)),\n",
    "                            system=system_prompt,\n",
    "                            messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                            temperature=temperature_step,\n",
    "                            top_p=1.0,\n",
    "                            stop_sequences=[]\n",
    "                        )\n",
    "                        string = \" \".join([block.text for block in response.content]).strip()\n",
    "                    \n",
    "                else:\n",
    "                    string = \"...\"  # Fallback if somehow we still get no response\n",
    "\n",
    "\n",
    "                string = truncate_text(string, response_length)\n",
    "                string = casual_text_formatting(string)\n",
    "                num_words = len(string.split())\n",
    "                cnt += 1\n",
    "            \n",
    "            speaker = f\"\\n{friend2}:\"\n",
    "            new_line = f\"<p> B: {string} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "            user_prompt += string + \"B: \"\n",
    "        \n",
    "        print(user_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Set up Anthropic Client\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "# Function to generate AI-like response lengths\n",
    "def generate_response_length():\n",
    "    return np.random.randint(3, 20)  # Ensure at least 3 words\n",
    "\n",
    "# Function to truncate text at a natural stopping point\n",
    "def truncate_text(text, max_words):\n",
    "    words = text.split()\n",
    "    if len(words) <= max_words:\n",
    "        return text\n",
    "    truncated_text = \" \".join(words[:max_words])\n",
    "    return re.sub(r'[^.!?]*$', '', truncated_text).strip()\n",
    "\n",
    "# Model and parameters\n",
    "model_A = \"claude-3-opus-20240229\"  # AI Agent A\n",
    "model_B = \"claude-3-5-sonnet-20241022\"  # AI Agent B (or use Opus for both)\n",
    "temperature = 0.6\n",
    "temperature_regen = 1\n",
    "\n",
    "list_topics = ['fashion', 'politics', 'books', 'sports', 'general entertainment', 'music', 'science', 'technology', 'movies', 'food']\n",
    "\n",
    "for conv in range(23, 24):\n",
    "    time.sleep(1)\n",
    "    filename = f'conv{conv}.html'\n",
    "    with open(filename, \"w\") as file_html:\n",
    "        topic = np.random.choice(list_topics)\n",
    "\n",
    "        personalityA = np.random.choice([\"logical and skeptical\", \"excitable and passionate\", \"sarcastic and witty\", \"serious and intellectual\"])\n",
    "        personalityB = np.random.choice([\"curious and open-minded\", \"pragmatic and reserved\", \"opinionated and strong-willed\", \"friendly and cheerful\"])\n",
    "\n",
    "        ### **ðŸ›  FIX: A and B Get Their Own System Prompts**\n",
    "        system_prompt_A = f\"A is {personalityA} and is having a natural conversation with a friend. A speaks naturally, responds informally, and keeps responses short. Do not use emojis or meta-analysis.\"\n",
    "        system_prompt_B = f\"B is {personalityB} and is having a natural conversation with a friend. B speaks naturally, responds informally, and keeps responses short. Do not use emojis or meta-analysis.\"\n",
    "\n",
    "        ### **ðŸ›  FIX: Store the Full Conversation Properly**\n",
    "        conversation_history = [\n",
    "            {\"role\": \"user\", \"content\": \"A: Hey there! What's up?\"}\n",
    "        ]\n",
    "\n",
    "        for i in range(24):  # 12 turns each\n",
    "            response_length = generate_response_length()\n",
    "            temperature_step = temperature + ((temperature_regen - temperature) / 5) * i\n",
    "\n",
    "            ### **ðŸ›  FIX: Alternate API Calls Properly**\n",
    "            if i % 2 == 0:\n",
    "                speaker = \"A\"\n",
    "                system_prompt = system_prompt_A\n",
    "                model = model_A\n",
    "                last_message = conversation_history[-1][\"content\"]  # Get the last response\n",
    "            else:\n",
    "                speaker = \"B\"\n",
    "                system_prompt = system_prompt_B\n",
    "                model = model_B\n",
    "                last_message = conversation_history[-1][\"content\"]  # Aâ€™s last response becomes Bâ€™s input\n",
    "\n",
    "            response = client.messages.create(\n",
    "                model=model,\n",
    "                max_tokens=80,\n",
    "                system=system_prompt,\n",
    "                messages=[{\"role\": \"user\", \"content\": last_message}],  # âœ… Feed previous AI response\n",
    "                temperature=temperature,\n",
    "                top_p=1.0,\n",
    "                stop_sequences=[]\n",
    "            )\n",
    "\n",
    "            if response.content:\n",
    "                print(f\"DEBUG: {speaker} Response: {response.content}\")\n",
    "                string = \" \".join([block.text for block in response.content]).strip()\n",
    "                if len(string.split()) < 3:  # Ensure at least 3 words\n",
    "                    response = client.messages.create(\n",
    "                        model=model,\n",
    "                        max_tokens=80,\n",
    "                        system=system_prompt,\n",
    "                        messages=[{\"role\": \"user\", \"content\": last_message}],  # âœ… Feed previous AI response\n",
    "                        temperature=temperature,\n",
    "                        top_p=1.0,\n",
    "                        stop_sequences=[]\n",
    "                    )\n",
    "                    string = \" \".join([block.text for block in response.content]).strip() if response.content else \"(No response)\"\n",
    "            else:\n",
    "                string = \"(No response)\"\n",
    "\n",
    "            string = truncate_text(string, response_length)\n",
    "\n",
    "            ### **ðŸ›  FIX: Store Response for Next AI Agent**\n",
    "            conversation_history.append({\"role\": \"user\", \"content\": f\"{speaker}: {string}\"})  # âœ… Save each AIâ€™s response\n",
    "\n",
    "            ### **ðŸ›  FIX: Write Proper Alternating Turns**\n",
    "            new_line = f\"<p> {speaker}: {string} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "\n",
    "        print(\"\\n\".join([msg[\"content\"] for msg in conversation_history]))  # âœ… Print full conversation history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Set up Anthropic Client\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "# Function to generate AI-like response lengths\n",
    "def generate_response_length():\n",
    "    return np.random.randint(3, 20)  # Ensure at least 3 words\n",
    "\n",
    "# Function to truncate text at a natural stopping point\n",
    "def truncate_text(text, max_words):\n",
    "    words = text.split()\n",
    "    if len(words) <= max_words:\n",
    "        return text\n",
    "    truncated_text = \" \".join(words[:max_words])\n",
    "    return re.sub(r'[^.!?]*$', '', truncated_text).strip()\n",
    "\n",
    "# Model and parameters\n",
    "model_A = \"claude-3-opus-20240229\"  # AI Agent A\n",
    "model_B = \"claude-3-5-sonnet-20241022\"  # AI Agent B (or use Opus for both)\n",
    "temperature = 0.6\n",
    "temperature_regen = 1\n",
    "\n",
    "list_topics = ['fashion', 'politics', 'books', 'sports', 'general entertainment', 'music', 'science', 'technology', 'movies', 'food']\n",
    "\n",
    "for conv in range(23, 24):\n",
    "    time.sleep(1)\n",
    "    filename = f'conv{conv}.html'\n",
    "    with open(filename, \"w\") as file_html:\n",
    "        topic = np.random.choice(list_topics)\n",
    "\n",
    "        personalityA = np.random.choice([\"logical and skeptical\", \"excitable and passionate\", \"sarcastic and witty\", \"serious and intellectual\"])\n",
    "        personalityB = np.random.choice([\"curious and open-minded\", \"pragmatic and reserved\", \"opinionated and strong-willed\", \"friendly and cheerful\"])\n",
    "\n",
    "        ### **ðŸ›  FIX: A and B Get Their Own System Prompts**\n",
    "        system_prompt_A = f\"A is {personalityA} and is having a natural conversation with a friend. A speaks naturally, responds informally, and keeps responses short. Do not use emojis or meta-analysis.\"\n",
    "        system_prompt_B = f\"B is {personalityB} and is having a natural conversation with a friend. B speaks naturally, responds informally, and keeps responses short. Do not use emojis or meta-analysis.\"\n",
    "\n",
    "        ### **ðŸ›  FIX: Store the Full Conversation Properly**\n",
    "        conversation_history = [\n",
    "            {\"role\": \"user\", \"content\": \"A: Hey there! What's up?\"}\n",
    "        ]\n",
    "\n",
    "        for i in range(24):  # 12 turns each\n",
    "            response_length = generate_response_length()\n",
    "            temperature_step = temperature + ((temperature_regen - temperature) / 5) * i\n",
    "\n",
    "            ### **ðŸ›  FIX: Alternate API Calls Properly**\n",
    "            if i % 2 == 0:\n",
    "                speaker = \"A\"\n",
    "                system_prompt = system_prompt_A\n",
    "                model = model_A\n",
    "                last_message = conversation_history[-1][\"content\"]  # Get the last response\n",
    "            else:\n",
    "                speaker = \"B\"\n",
    "                system_prompt = system_prompt_B\n",
    "                model = model_B\n",
    "                last_message = conversation_history[-1][\"content\"]  # Aâ€™s last response becomes Bâ€™s input\n",
    "\n",
    "            response = client.messages.create(\n",
    "                model=model,\n",
    "                max_tokens=80,\n",
    "                system=system_prompt,\n",
    "                messages=[{\"role\": \"user\", \"content\": last_message}],  # âœ… Feed previous AI response\n",
    "                temperature=temperature,\n",
    "                top_p=1.0,\n",
    "                stop_sequences=[]\n",
    "            )\n",
    "\n",
    "            if response.content:\n",
    "                print(f\"DEBUG: {speaker} Response: {response.content}\")\n",
    "                string = \" \".join([block.text for block in response.content]).strip()\n",
    "                if len(string.split()) < 3:  # Ensure at least 3 words\n",
    "                    response = client.messages.create(\n",
    "                        model=model,\n",
    "                        max_tokens=80,\n",
    "                        system=system_prompt,\n",
    "                        messages=[{\"role\": \"user\", \"content\": last_message}],  # âœ… Feed previous AI response\n",
    "                        temperature=temperature,\n",
    "                        top_p=1.0,\n",
    "                        stop_sequences=[]\n",
    "                    )\n",
    "                    string = \" \".join([block.text for block in response.content]).strip() if response.content else \"(No response)\"\n",
    "            else:\n",
    "                string = \"(No response)\"\n",
    "\n",
    "            string = truncate_text(string, response_length)\n",
    "\n",
    "            ### **ðŸ›  FIX: Store Response for Next AI Agent**\n",
    "            conversation_history.append({\"role\": \"user\", \"content\": f\"{speaker}: {string}\"})  # âœ… Save each AIâ€™s response\n",
    "\n",
    "            ### **ðŸ›  FIX: Write Proper Alternating Turns**\n",
    "            new_line = f\"<p> {speaker}: {string} </p>\\n\"\n",
    "            file_html.write(new_line)\n",
    "\n",
    "        print(\"\\n\".join([msg[\"content\"] for msg in conversation_history]))  # âœ… Print full conversation history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
