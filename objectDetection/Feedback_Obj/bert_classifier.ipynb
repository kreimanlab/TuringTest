{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b62cf644e84491693383562fb2d3fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bd5eb32a3341cea8ae02af516e8205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767e38f613164a20984fb51e1ba518f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2c881e020f4b5ba111fcfd0dafcff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249211fca406458bb2e2477c67f65795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(allowed_algorithms = ['azure','rekognition','detectron','google']):\n",
    "    with open('multi_label_compiled_with_humans.p','rb') as F:\n",
    "        multi_label_compiled_with_humans = pickle.load(F)\n",
    "    print(multi_label_compiled_with_humans)\n",
    "    human_data = []\n",
    "    ai_data = []\n",
    "    data_images = []\n",
    "    for key in multi_label_compiled_with_humans.keys():\n",
    "        data_images.append(key)\n",
    "        data = multi_label_compiled_with_humans[key]\n",
    "        human_data.append(' '.join(data['humans'][0]))\n",
    "        \n",
    "        for algo in allowed_algorithms:\n",
    "            ai_data.append(' '.join(data[algo]))\n",
    "    return human_data, ai_data, data_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_data(human_data, ai_data):\n",
    "    ids = list(range(len(human_data)))\n",
    "    cutoff_id = int(0.9*len(ids))\n",
    "    random.shuffle(ids)\n",
    "    sentences_train = {}\n",
    "    sentences_val = {}\n",
    "  \n",
    "\n",
    "    for train_id in ids[:cutoff_id]:\n",
    "        data = human_data[train_id]\n",
    "        sentences_train[data] = 0\n",
    "        ai_data_ = ai_data[train_id]\n",
    "        if ai_data_ in sentences_val.keys():\n",
    "            ai_data_ = 'NA'%ai_data_\n",
    "        sentences_train[ai_data_] = 1\n",
    "        \n",
    "        \n",
    "    print(len(sentences_val.keys()))\n",
    "    for test_id in ids[cutoff_id:]:\n",
    "        data = human_data[test_id]\n",
    "        sentences_val[data] = 0\n",
    "        ai_data_ = ai_data[test_id]\n",
    "        if ai_data_ in sentences_val.keys():\n",
    "            print(ai_data_)\n",
    "            ai_data_ =  '%s_%s'%(ai_data_,test_id)\n",
    "        sentences_val[ai_data_] = 1\n",
    "    print(len(sentences_val.keys()))\n",
    "\n",
    "        \n",
    "    tokenized_train = tokenizer(list(sentences_train.keys()), padding = True, truncation = True, return_tensors=\"pt\")\n",
    "    tokenized_val = tokenizer(list(sentences_val.keys()) , padding = True, truncation = True,  return_tensors=\"pt\")\n",
    "    \n",
    "\n",
    "    #move on device (GPU)\n",
    "    tokenized_train = {k:torch.tensor(v).to(device) for k,v in tokenized_train.items()}\n",
    "    tokenized_val = {k:torch.tensor(v).to(device) for k,v in tokenized_val.items()}\n",
    "    \n",
    "    LE = LabelEncoder()\n",
    "    labels_train = LE.fit_transform(list(sentences_train.values()))\n",
    "    labels_val = LE.fit_transform(list(sentences_val.values()))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden_train = model(**tokenized_train) #dim : [batch_size(nr_sentences), tokens, emb_dim]\n",
    "        hidden_val = model(**tokenized_val)\n",
    "\n",
    "    #get only the [CLS] hidden states\n",
    "    cls_train = hidden_train.last_hidden_state[:,0,:]\n",
    "    cls_val = hidden_val.last_hidden_state[:,0,:]\n",
    "    \n",
    "    x_train = cls_train.to(\"cpu\")\n",
    "    y_train = labels_train\n",
    "\n",
    "    x_val = cls_val.to(\"cpu\")\n",
    "    y_val = labels_val\n",
    "    \n",
    "    \n",
    "    # Randomize test data\n",
    "#     temp = list(zip(x_val, y_val))\n",
    "#     random.shuffle(temp)\n",
    "#     x_val, y_val = zip(*temp)\n",
    "#     x_val, y_val = list(x_val), list(y_val)\n",
    "\n",
    "#     y_val = np.array(y_val)\n",
    "#     x_val = torch.vstack(x_val)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_score(algo = 'SVM'):\n",
    "    if algo == 'RF':\n",
    "        rf = RandomForestClassifier()\n",
    "        rf.fit(x_train,y_train)\n",
    "        predictions = rf.predict(x_val)\n",
    "        score = rf.score(x_val,y_val) \n",
    "    elif algo == 'SVM':\n",
    "        svm = SVC()\n",
    "        svm.fit(x_train,y_train)\n",
    "        predictions = svm.predict(x_val)\n",
    "        score = svm.score(x_val,y_val) \n",
    "        \n",
    "    conf_mat = confusion_matrix(y_val, predictions,normalize='true')\n",
    "    return conf_mat, score, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracies_M = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracies_M['all'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_conf_mat = np.zeros((6,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data, ai_data,data_images = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_data_sampled = []\n",
    "for i in range(0,len(ai_data),4):\n",
    "    random_offset = random.choice([0,1,2,3])\n",
    "    ai_data_sampled.append(ai_data[i+random_offset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Person Human Clothing\n",
      "Room Indoors Toilet\n",
      "162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30789/4140100945.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenized_train = {k:torch.tensor(v).to(device) for k,v in tokenized_train.items()}\n",
      "/tmp/ipykernel_30789/4140100945.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenized_val = {k:torch.tensor(v).to(device) for k,v in tokenized_val.items()}\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_val,y_val,ids = embed_data(human_data, ai_data_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_conf, svm_score,predictions = classify_and_score('SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_id = int(0.9*len(ids))\n",
    "test_images = []\n",
    "for test_id in ids[cutoff_id:]:\n",
    "    test_image = data_images[test_id]\n",
    "    test_images.append(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "for im_ct in range(len(test_images)):\n",
    "    test_image = test_images[im_ct]\n",
    "    pred_1 = predictions[im_ct]\n",
    "    gt_1 = y_val[im_ct]\n",
    "    \n",
    "    pred_2 = predictions[im_ct + 81]\n",
    "    gt_2 = y_val[im_ct + 81]\n",
    "    all_accuracies_M['all'][test_image] = [pred_1 == gt_1]\n",
    "    all_accuracies_M['all'][test_image].append(pred_2 == gt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_conf_mat[0] = svm_conf[0]\n",
    "result_conf_mat[1] = svm_conf[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humans vs Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracies_M['azure'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "animal mammal outdoor\n",
      "person clothing human face\n",
      "person outdoor clothing\n",
      "162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30789/4140100945.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenized_train = {k:torch.tensor(v).to(device) for k,v in tokenized_train.items()}\n",
      "/tmp/ipykernel_30789/4140100945.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenized_val = {k:torch.tensor(v).to(device) for k,v in tokenized_val.items()}\n"
     ]
    }
   ],
   "source": [
    "human_data, ai_data,data_images = read_data(['azure'])\n",
    "x_train,y_train,x_val,y_val,ids = embed_data(human_data, ai_data)\n",
    "svm_conf, svm_score,predictions = classify_and_score('SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9135802469135802"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_id = int(0.9*len(ids))\n",
    "test_images = []\n",
    "for test_id in ids[cutoff_id:]:\n",
    "    test_image = data_images[test_id]\n",
    "    test_images.append(test_image)\n",
    "\n",
    "for im_ct in range(len(test_images)):\n",
    "    test_image = test_images[im_ct]\n",
    "    pred_1 = predictions[im_ct]\n",
    "    gt_1 = y_val[im_ct]\n",
    "    \n",
    "    pred_2 = predictions[im_ct + 81]\n",
    "    gt_2 = y_val[im_ct + 81]\n",
    "    all_accuracies_M['azure'][test_image] = [pred_1 == gt_1]\n",
    "    all_accuracies_M['azure'][test_image].append(pred_2 == gt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9135802469135802"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_conf_mat[2] = svm_conf[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humans vs Detectron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracies_M['detectron'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "person kite\n",
      "train\n",
      "car bus person\n",
      "zebra\n",
      "162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30789/4140100945.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenized_train = {k:torch.tensor(v).to(device) for k,v in tokenized_train.items()}\n",
      "/tmp/ipykernel_30789/4140100945.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenized_val = {k:torch.tensor(v).to(device) for k,v in tokenized_val.items()}\n"
     ]
    }
   ],
   "source": [
    "human_data, ai_data,data_images = read_data(['detectron'])\n",
    "x_train,y_train,x_val,y_val,ids = embed_data(human_data, ai_data)\n",
    "svm_conf, svm_score,predictions = classify_and_score('SVM')\n",
    "# print('RF accuracy:%s'%rf_score)\n",
    "# print('RF confusion:%s'%rf_conf)\n",
    "\n",
    "result_conf_mat[3] = svm_conf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_id = int(0.9*len(ids))\n",
    "test_images = []\n",
    "for test_id in ids[cutoff_id:]:\n",
    "    test_image = data_images[test_id]\n",
    "    test_images.append(test_image)\n",
    "\n",
    "for im_ct in range(len(test_images)):\n",
    "    test_image = test_images[im_ct]\n",
    "    pred_1 = predictions[im_ct]\n",
    "    gt_1 = y_val[im_ct]\n",
    "    \n",
    "    pred_2 = predictions[im_ct + 81]\n",
    "    gt_2 = y_val[im_ct + 81]\n",
    "    all_accuracies_M['detectron'][test_image] = [pred_1 == gt_1]\n",
    "    all_accuracies_M['detectron'][test_image].append(pred_2 == gt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humans vs Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracies_M['google'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Food Tableware Plate\n",
      "Food Ingredient Recipe\n",
      "162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30789/4140100945.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenized_train = {k:torch.tensor(v).to(device) for k,v in tokenized_train.items()}\n",
      "/tmp/ipykernel_30789/4140100945.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenized_val = {k:torch.tensor(v).to(device) for k,v in tokenized_val.items()}\n"
     ]
    }
   ],
   "source": [
    "human_data, ai_data, data_images = read_data(['google'])\n",
    "x_train,y_train,x_val,y_val,ids = embed_data(human_data, ai_data)\n",
    "svm_conf, svm_score,predictions = classify_and_score('SVM')\n",
    "# print('RF accuracy:%s'%rf_score)\n",
    "# print('RF confusion:%s'%rf_conf)\n",
    "\n",
    "result_conf_mat[4] = svm_conf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_id = int(0.9*len(ids))\n",
    "test_images = []\n",
    "for test_id in ids[cutoff_id:]:\n",
    "    test_image = data_images[test_id]\n",
    "    test_images.append(test_image)\n",
    "\n",
    "for im_ct in range(len(test_images)):\n",
    "    test_image = test_images[im_ct]\n",
    "    pred_1 = predictions[im_ct]\n",
    "    gt_1 = y_val[im_ct]\n",
    "    \n",
    "    pred_2 = predictions[im_ct + 81]\n",
    "    gt_2 = y_val[im_ct + 81]\n",
    "    all_accuracies_M['google'][test_image] = [pred_1 == gt_1]\n",
    "    all_accuracies_M['google'][test_image].append(pred_2 == gt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humans vs Rekognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracies_M['rekognition'] = {}all_accuracies_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Person Human Tennis\n",
      "Bus Vehicle Transportation\n",
      "Person Human Skateboard\n",
      "Person Human Tennis Racket\n",
      "Person Human Skateboard\n",
      "Person Human Skateboard\n",
      "Person Human Truck\n",
      "Plant Vegetable Food\n",
      "Person Human Tennis\n",
      "162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30789/4140100945.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenized_train = {k:torch.tensor(v).to(device) for k,v in tokenized_train.items()}\n",
      "/tmp/ipykernel_30789/4140100945.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenized_val = {k:torch.tensor(v).to(device) for k,v in tokenized_val.items()}\n"
     ]
    }
   ],
   "source": [
    "human_data, ai_data,data_images = read_data(['rekognition'])\n",
    "x_train,y_train,x_val,y_val,ids = embed_data(human_data, ai_data)\n",
    "svm_conf, svm_score,predictions = classify_and_score('SVM')\n",
    "# print('RF accuracy:%s'%rf_score)\n",
    "# print('RF confusion:%s'%rf_conf)\n",
    "\n",
    "result_conf_mat[5] = svm_conf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_id = int(0.9*len(ids))\n",
    "test_images = []\n",
    "for test_id in ids[cutoff_id:]:\n",
    "    test_image = data_images[test_id]\n",
    "    test_images.append(test_image)\n",
    "\n",
    "for im_ct in range(len(test_images)):\n",
    "    test_image = test_images[im_ct]\n",
    "    pred_1 = predictions[im_ct]\n",
    "    gt_1 = y_val[im_ct]\n",
    "    \n",
    "    pred_2 = predictions[im_ct + 81]\n",
    "    gt_2 = y_val[im_ct + 81]\n",
    "    all_accuracies_M['rekognition'][test_image] = [pred_1 == gt_1]\n",
    "    all_accuracies_M['rekognition'][test_image].append(pred_2 == gt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('all_machine_data.p','wb') as F:\n",
    "#     pickle.dump(all_accuracies_M,F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 86.4x252 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelist = ['Azure','Detectron','Google','Rekognition']\n",
    "\n",
    "plotname = 'mutli_label_recognition'\n",
    "\n",
    "import os\n",
    "os.makedirs('figures',exist_ok=True)\n",
    "\n",
    "#plot confusion matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm = pd.DataFrame(result_conf_mat, index = [i for i in ['Human','AI']+modelist],\n",
    "                  columns = [i for i in ['Human','AI']])\n",
    "plt.figure(figsize = (1.2,3.5))\n",
    "sn.heatmap(df_cm, annot=True,robust=True, cmap='RdBu_r', vmin=0, vmax=1)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Ground Truth\")\n",
    "\n",
    "plt.savefig('figures/' + plotname + '_bert_confmat.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STD calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostrap(data1, data2):\n",
    "    import random\n",
    "    import math\n",
    "    store = []\n",
    "    sample_mean = []\n",
    "    if len(data1) < len(data2):\n",
    "        SampleTimes = math.ceil(len(data1)/2)\n",
    "        BootstrapTimes = len(data1)\n",
    "    else:\n",
    "        SampleTimes = math.ceil(len(data2)/2)\n",
    "        BootstrapTimes = len(data2)\n",
    "        \n",
    "    #print(BootstrapTimes)        \n",
    "    for i in range(BootstrapTimes):\n",
    "        y1 = random.sample(data1, SampleTimes)\n",
    "        #print(y1)\n",
    "        y2 = random.sample(data2, SampleTimes)\n",
    "        #print(y2)\n",
    "        y_comb = y1+y2\n",
    "        #print(y_comb)\n",
    "        sample_mean.append(np.mean(np.array(y_comb), axis=0))\n",
    "    \n",
    "    stdval = np.std(np.array(sample_mean), axis=0)\n",
    "    return stdval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stds = []\n",
    "for split in ['all','azure','detectron','google','rekognition']:\n",
    "    data_1 = []\n",
    "    data_2 = []\n",
    "    for im in all_accuracies_M[split].keys():\n",
    "        if all_accuracies_M[split][im][0] == True:\n",
    "            data_1.append(1)\n",
    "        else:\n",
    "            data_1.append(0)\n",
    "\n",
    "        if all_accuracies_M[split][im][1] == True:\n",
    "            data_2.append(1)\n",
    "        else:\n",
    "            data_2.append(0)\n",
    "    stdval = boostrap(data_1,data_2)\n",
    "    all_stds.append(stdval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Azure', 'Detectron', 'Google', 'Rekognition']"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.88888889 0.11111111]\n",
      " [0.18518519 0.81481481]\n",
      " [0.13580247 0.86419753]\n",
      " [0.28395062 0.71604938]\n",
      " [0.19753086 0.80246914]\n",
      " [0.25925926 0.74074074]]\n",
      "[0.8518518518518519, 0.8765432098765431, 0.8024691358024691, 0.845679012345679, 0.8148148148148148]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#compute top 1 accuracy for each AI model and also overall AI\n",
    "top1 = []\n",
    "print(result_conf_mat)\n",
    "for i in range(1+len(modelist)):\n",
    "    #print(i)\n",
    "    top1.append((result_conf_mat[0][0]+result_conf_mat[i+1][1])/2)\n",
    "print(top1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "#data\n",
    "#x-axis\n",
    "years = list(range(1, 2+len(modelist)))\n",
    "strtask = ['AI'] + modelist\n",
    "#print(years)\n",
    "#print(strtask)\n",
    "\n",
    "# Figure Size\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "ax.add_patch(Rectangle((-0.5, 0.45), len(top1), 0.1,facecolor='yellow'))\n",
    "\n",
    "#bar chart properties\n",
    "# ax.bar(strtask, top1, color ='black', width = 0.3)\n",
    "ax.bar(strtask, top1, yerr=all_stds,\n",
    "       align='center', alpha=0.5, ecolor='black', capsize=10,\n",
    "       color ='black', width = 0.3)\n",
    "\n",
    "plt.ylabel('Top-1 Accuracy')\n",
    "plt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees\n",
    "\n",
    "plt.ylim(0.3, 1)\n",
    "#plt.xlim(0.5, len(years)+0.5)\n",
    "\n",
    "#draw chance 0.5 \n",
    "# chancex =np.arange(len(years))\n",
    "# plt.plot(chancex, np.arange(len(chancex))*0+0.5, 'k--', label='chance (50%)')\n",
    "#plt.legend()\n",
    " \n",
    "fig.tight_layout()\n",
    "\n",
    "fig.set_figwidth(2.5)\n",
    "fig.set_figheight(5)\n",
    "\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.55, 1.0), ncol=3, fancybox=True, shadow=True,frameon=False)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('figures/' + plotname + '_bert.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
