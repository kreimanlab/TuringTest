{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0239ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def FindEducationTage(edu):\n",
    "    if edu == \"High School\":\n",
    "        edutag = 'high school'\n",
    "    elif edu == 'Middle School':\n",
    "        edutag = 'middle school'\n",
    "    elif edu == 'Bachelors Degree':\n",
    "        edutag = 'bachelor'\n",
    "    elif edu == 'Masters Degree':\n",
    "        edutag = 'master'\n",
    "    elif edu == 'Post-Graduate Degree':\n",
    "        edutag = 'pg'\n",
    "    else:\n",
    "        raise Exception(\"Education Tag undefined: \" + edu)\n",
    "        \n",
    "    return edutag\n",
    "\n",
    "\n",
    "def FindGenderTage(gender):\n",
    "    if gender == \"Male\":\n",
    "        gendertag = 'M'\n",
    "    elif gender == 'Female':\n",
    "        gendertag = 'F'    \n",
    "    else:\n",
    "        raise Exception(\"Gender Tag undefined: \" + gender)\n",
    "    return gendertag\n",
    "\n",
    "\n",
    "def FindHitLen(convname):\n",
    "    hitid = int(convname[re.search(r'static/dataset/conv', convname).end():re.search(r'_len', convname).start()])\n",
    "    convlen = int(convname[re.search(r'_len', convname).end():re.search(r'.html', convname).start()])\n",
    "    return hitid, convlen\n",
    "\n",
    "\n",
    "def FindTopicChoice(optionlist, topicsel):\n",
    "    count = 0\n",
    "    for to in optionlist:\n",
    "        if to == topicsel:\n",
    "            return count+1\n",
    "        else:\n",
    "            count = count + 1\n",
    "    if count == 0:\n",
    "        raise Exception(\"TopicChoice undefined\")\n",
    "        \n",
    "        \n",
    "def FindTuringResTage(response):\n",
    "    if response == 'A= Human, B = Human':\n",
    "        restag = 'HH'\n",
    "    elif response == 'A= Human, B = Machine':\n",
    "        restag = 'HAI'\n",
    "    elif response == 'A= Machine, B = Human':\n",
    "        restag = 'AIH'\n",
    "    elif response == 'A= Machine, B = Machine':\n",
    "        restag = 'AIAI'\n",
    "    else:\n",
    "        raise Exception(\"TuringRes Tag undefined\")\n",
    "        \n",
    "    return restag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sorted the data based on the following criteria in the journal:\n",
    "##\\MM{other tasks: main text: combine + supp: in-lab} \n",
    "##\\MM{conversation: main text: in lab + supp: mturk + prolific}\n",
    "\n",
    "results = {'inlab':{'datastring':[]}, 'prolific':{'datastring':[]}, 'mturk':{'datastring':[]}, 'chatgpt':{'datastring':[]}}\n",
    "\n",
    "\n",
    "############# for jason file processing\n",
    "jsonpath = './Data_all/'\n",
    "filenames = {'inlab': ['Marcelo_InLab_With_Gender_3','Prachi_InLab_With_Gender_2','Prachi_InLab_Without_Gender_2',\n",
    "                      'Shui_InLab_With_Gender_16','Shui_InLab_Without_Gender_17', 'Tanish_InLab_WithGender'],\n",
    "             'prolific':['prolific_withgender50participants','prolific_withoutgender52participants']}\n",
    "\n",
    "for key in filenames:\n",
    "    print(\"=======================\")\n",
    "    print(key)\n",
    "\n",
    "    for jasonfilename in filenames[key]:\n",
    "        #print(\"=======================\")\n",
    "        #print(jasonfilename)\n",
    "        f = open(jsonpath + jasonfilename + '.json')\n",
    "        data = json.load(f)\n",
    "        #print(len(data))\n",
    "        for workerid in range(len(data)):\n",
    "            #print(data[workerid])\n",
    "            demographic_trialid = 0\n",
    "            while not ('rawResponses' in data[workerid][demographic_trialid]):\n",
    "                demographic_trialid = demographic_trialid + 1\n",
    "            \n",
    "            workerdata = {'questiondata':{'gender':0,'age':0,'native':0,'education':0,'country':0},\n",
    "                          'data':[{'trialdata':{'hit':0,'topic':0,'hit_len':0,'Aclass':0,'rt':0,\n",
    "                                                'Agender': 0, 'Bgender': 0, 'conversationID': '.html'}}]}\n",
    "\n",
    "            workerdata['questiondata']['gender'] = data[workerid][demographic_trialid]['rawResponses']['A1'].lower()\n",
    "            workerdata['questiondata']['country'] = data[workerid][demographic_trialid]['rawResponses']['A3']\n",
    "            workerdata['questiondata']['age'] = data[workerid][demographic_trialid]['rawResponses']['A0'][:-10]\n",
    "            workerdata['questiondata']['native'] = \"yes\"\n",
    "            workerdata['questiondata']['education'] = FindEducationTage(data[workerid][demographic_trialid]['rawResponses']['A2'])\n",
    "            \n",
    "            official_trialid = demographic_trialid + 1\n",
    "            while not ('conversation_name1' in data[workerid][official_trialid]):\n",
    "                official_trialid = official_trialid + 1\n",
    "            \n",
    "            for trialid in range(official_trialid,len(data[workerid])-2): #starting from 5!!! the first few are not valid trials\n",
    "                trialdata = {'trialdata':{'hit':0,'topic':0,'hit_len':0,'Aclass':0,'rt':0,\n",
    "                                                'Agender': 0, 'Bgender': 0, 'conversationID': '.html'}}\n",
    "\n",
    "                #print(\"=========================\")\n",
    "                #print(data[workerid][trialid])\n",
    "                trialdata['trialdata']['rt'] = data[workerid][trialid]['rt']\n",
    "                convname = data[workerid][trialid]['conversation_name1']\n",
    "                trialdata['trialdata']['conversationID'] = convname\n",
    "                hitid, convlen = FindHitLen(convname)\n",
    "                trialdata['trialdata']['hit'] = hitid\n",
    "                trialdata['trialdata']['hit_len'] = convlen\n",
    "                optionlist = [data[workerid][trialid]['optionslistone'], \n",
    "                             data[workerid][trialid]['optionslisttwo'],\n",
    "                             data[workerid][trialid]['optionslistthree'],\n",
    "                             data[workerid][trialid]['optionslistfour'],\n",
    "                             data[workerid][trialid]['optionslistfive']]\n",
    "                topicsel = data[workerid][trialid]['response']['topic']\n",
    "                topic = FindTopicChoice(optionlist, topicsel)\n",
    "                trialdata['trialdata']['topic'] = str(topic)\n",
    "                \n",
    "                if 'gender1' in data[workerid][trialid]['response']:\n",
    "                    trialdata['trialdata']['Agender'] = FindGenderTage(data[workerid][trialid]['response']['gender1'])\n",
    "                    trialdata['trialdata']['Bgender'] = FindGenderTage(data[workerid][trialid]['response']['gender2'])\n",
    "                \n",
    "                trialdata['trialdata']['Aclass'] = FindTuringResTage(data[workerid][trialid]['response']['type1'])\n",
    "                #print(\"=========================\")\n",
    "                print(trialdata)\n",
    "                workerdata['data'].append(trialdata)\n",
    "\n",
    "            #print(\"=========================\")\n",
    "            #print(workerdata)\n",
    "            results[key]['datastring'].append(workerdata)\n",
    "            \n",
    "print(\"=========================\")\n",
    "print(\"number of in-lab pariticpants:\")\n",
    "print(len(results['inlab']['datastring']))\n",
    "print(\"number of prolific pariticpants:\")\n",
    "print(len(results['prolific']['datastring']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad772d",
   "metadata": {},
   "source": [
    "What Elisa used for her Mturk with feedback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b93cfafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3723be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables inside the database: [('apscheduler_jobs',), ('TuringConversationalAI',), ('amt_hit',), ('campaign',)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = {'prolific':{'datastring':[]}}\n",
    "\n",
    "# Connect to your database\n",
    "db_path = r\"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/Data_all/participants_prolific_may30.db\"\n",
    "con = sqlite3.connect(db_path)\n",
    "\n",
    "# List all tables\n",
    "cursor = con.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "print(\"Tables inside the database:\", tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca4767",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'AIexperts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5y/60d837ps00s6_lbr07dlqtwh0000gn/T/ipykernel_1088/974670138.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mworkerdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrialdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datastring'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkerdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=========================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'AIexperts'"
     ]
    }
   ],
   "source": [
    "############# for db file processing \n",
    "file_path = db_path #r\"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/conversation_task/participants_AIexperts.db\"\n",
    "con = sqlite3.connect(file_path)\n",
    "# Load the data into a DataFrame\n",
    "df = pd.read_sql_query(\"SELECT * from TuringConversationalAI\", con)\n",
    "# = sqlite3.connect('./Data_all/Giorgia_all_inlab_online_2.db')\n",
    "# Load the data into a DataFrame\n",
    "#df2 = pd.read_sql_query(\"SELECT * from TuringConversationalAI\", con)\n",
    "#df = pd.concat([df1,df2], ignore_index=True)\n",
    "\n",
    "num_conv = 32\n",
    "key = 'prolific'\n",
    "for w in range(len(df)):\n",
    "    if df[\"mode\"][w] in [\"debug\"] and df[\"status\"][w] == 3 and df[\"datastring\"][w] is not None:\n",
    "        #data = json.loads(df.loc[w].at[\"datastring\"])\n",
    "        \n",
    "        workerdata = {'questiondata':{},\n",
    "                          'data':[]}\n",
    "        \n",
    "        data = json.loads(df[\"datastring\"][w])\n",
    "        Wgender = data['questiondata']['gender']\n",
    "        Wage = data['questiondata']['age']\n",
    "        Wlanguage = data['questiondata']['native']\n",
    "        Wdegree = data['questiondata']['education']\n",
    "        Wfield = data['questiondata']['country']        \n",
    "        \n",
    "        workerdata['questiondata']['gender'] = Wgender\n",
    "        workerdata['questiondata']['age'] = Wage\n",
    "        workerdata['questiondata']['native'] = Wlanguage\n",
    "        workerdata['questiondata']['education'] = Wdegree\n",
    "        workerdata['questiondata']['country'] = Wfield       \n",
    "        \n",
    "        for c in range(num_conv):\n",
    "            \n",
    "            trialdata = {'trialdata':{'hit':0,'topic':0,'hit_len':0,'Aclass':0,'rt':0,\n",
    "                                                'Agender': 0, 'Bgender': 0, 'conversationID': '.html'}}\n",
    "            \n",
    "            #print(data['data'][1+c]['trialdata'])\n",
    "            # load responses to conv1\n",
    "            conv1_hit = data['data'][2+c]['trialdata']['hit']\n",
    "            conv1_len = data['data'][2+c]['trialdata']['hit_len']\n",
    "            conv1_combclass = data['data'][2+c]['trialdata']['Aclass']\n",
    "            conv1_topic = data['data'][2+c]['trialdata']['topic']\n",
    "            conv1_convname = 'static/dataset/conv'+str(conv1_hit)+'_len'+str(conv1_len)+'.html'\n",
    "            conv1_rt = data['data'][2+c]['trialdata']['rt'] # absolute\n",
    "            conv1_Agender = data['data'][2+c]['trialdata']['Agender']\n",
    "            conv1_Bgender = data['data'][2+c]['trialdata']['Bgender']\n",
    "            is_catch = data['data'][2+c]['trialdata']['isCatch']\n",
    "            conv1_hasFeedback = data['data'][2+c]['trialdata']['hasFeedback']\n",
    "            \n",
    "            trialdata['trialdata']['rt'] = conv1_rt\n",
    "            trialdata['trialdata']['hit'] = conv1_hit\n",
    "            trialdata['trialdata']['topic'] = conv1_topic\n",
    "            trialdata['trialdata']['hit_len'] = conv1_len\n",
    "            trialdata['trialdata']['Aclass'] = conv1_combclass\n",
    "            trialdata['trialdata']['Agender'] = conv1_Agender\n",
    "            trialdata['trialdata']['Bgender'] = conv1_Bgender\n",
    "            trialdata['trialdata']['conversationID'] = conv1_convname\n",
    "            trialdata['trialdata']['isCatch'] = is_catch\n",
    "            trialdata['trialdata']['hasFeedback'] = conv1_hasFeedback\n",
    "            \n",
    "            workerdata['data'].append(trialdata)\n",
    "        results[key]['datastring'].append(workerdata)\n",
    "        \n",
    "print(\"=========================\")\n",
    "print(\"number of prolific pariticpants:\")\n",
    "print(len(results['prolific']['datastring']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ba896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the compiled results as jason\n",
    "with open(\"./savedResults_elisa/compiled_prolific.json\", \"w\") as fp:\n",
    "    json.dump(results,fp) \n",
    "    \n",
    "print('Compilation completed! Results are saved to folder!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17216c0",
   "metadata": {},
   "source": [
    "# Preprocess for confusion matrices divided by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f33fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_topic_mapping(excel_path, sheet_name=None):\n",
    "    \"\"\"\n",
    "    Load the Excel sheet and create a dictionary mapping conversation_id to topic\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(excel_path, sheet_name='Sheet1')\n",
    "    topic_mapping = {}\n",
    "    for _, row in df.iterrows():\n",
    "        conv_id = str(int(row.iloc[0]))  # First column (conversation_id)\n",
    "        topic = row.iloc[33] if len(row) > 33 else None  # Column AH (34th column, 0-indexed = 33)\n",
    "        if pd.notna(topic):\n",
    "            topic_mapping[conv_id] = topic.lower().strip()\n",
    "    return topic_mapping\n",
    "\n",
    "def reorganize_by_topic(compiled_json_path, excel_path, output_path=\"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/compiled_by_topic.json\"):\n",
    "    \"\"\"\n",
    "    Reorganize existing compiled JSON by topic using Excel mapping\n",
    "    \"\"\"\n",
    "    # Load topic mapping\n",
    "    topic_mapping = load_topic_mapping(excel_path)\n",
    "    print(f\"Loaded {len(topic_mapping)} conversation-topic mappings\")\n",
    "    \n",
    "    # Load existing compiled JSON\n",
    "    with open(compiled_json_path, 'r') as f:\n",
    "        original_data = json.load(f)\n",
    "    \n",
    "    # Initialize results structure by topic - preserve original source structure\n",
    "    topics = ['science', 'technology', 'books', 'movies', 'music', 'politics', \n",
    "              'general entertainment', 'fashion', 'sports', 'food']\n",
    "    \n",
    "    # Get source keys from original data (inlab, prolific, etc.)\n",
    "    source_keys = [key for key in original_data.keys() if 'datastring' in original_data.get(key, {})]\n",
    "    \n",
    "    results_by_topic = {}\n",
    "    for topic in topics:\n",
    "        results_by_topic[topic] = {}\n",
    "        for source_key in source_keys:\n",
    "            results_by_topic[topic][source_key] = {'datastring': []}\n",
    "    \n",
    "    # Also keep original structure for reference\n",
    "    results_by_topic['all'] = original_data\n",
    "    \n",
    "    # Process each data source (inlab, prolific, etc.)\n",
    "    for source_key in source_keys:\n",
    "        print(f\"\\nProcessing {source_key} data...\")\n",
    "        \n",
    "        for participant in original_data[source_key]['datastring']:\n",
    "            # Group this participant's trials by topic\n",
    "            trials_by_topic = {}\n",
    "            \n",
    "            for trial in participant['data']:\n",
    "                if 'trialdata' not in trial:\n",
    "                    continue\n",
    "                    \n",
    "                # Get conversation ID (hit number)\n",
    "                hit_id = str(trial['trialdata']['hit'])\n",
    "                \n",
    "                # Look up topic from mapping\n",
    "                assigned_topic = topic_mapping.get(hit_id, 'nolabel')\n",
    "                \n",
    "                # Add assigned_topic to trial data\n",
    "                trial['trialdata']['assigned_topic'] = assigned_topic\n",
    "                \n",
    "                # Group by topic\n",
    "                if assigned_topic not in trials_by_topic:\n",
    "                    trials_by_topic[assigned_topic] = []\n",
    "                trials_by_topic[assigned_topic].append(trial)\n",
    "            \n",
    "            # Create separate participant entries for each topic they have trials for\n",
    "            for topic, topic_trials in trials_by_topic.items():\n",
    "                if topic_trials and topic in results_by_topic:\n",
    "                    participant_topic_data = {\n",
    "                        'questiondata': participant['questiondata'].copy(),\n",
    "                        'data': topic_trials\n",
    "                    }\n",
    "                    results_by_topic[topic][source_key]['datastring'].append(participant_topic_data)\n",
    "    \n",
    "    # Save reorganized data\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(results_by_topic, f, indent=2)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n=========================\")\n",
    "    print(\"REORGANIZATION COMPLETE\")\n",
    "    print(f\"Output saved to: {output_path}\")\n",
    "    print(\"=========================\")\n",
    "    print(\"SUMMARY BY TOPIC:\")\n",
    "    print(\"=========================\")\n",
    "    \n",
    "    for topic in topics:\n",
    "        total_count = sum(len(results_by_topic[topic][source]['datastring']) \n",
    "                         for source in results_by_topic[topic])\n",
    "        if total_count > 0:\n",
    "            breakdown = []\n",
    "            for source in results_by_topic[topic]:\n",
    "                count = len(results_by_topic[topic][source]['datastring'])\n",
    "                if count > 0:\n",
    "                    breakdown.append(f\"{source}: {count}\")\n",
    "            breakdown_str = \", \".join(breakdown)\n",
    "            print(f\"{topic.upper()}: {total_count} participants ({breakdown_str})\")\n",
    "    \n",
    "    total_original = sum(len(original_data[source]['datastring']) \n",
    "                        for source in source_keys)\n",
    "    print(f\"\\nOriginal total participants: {total_original}\")\n",
    "    \n",
    "    return results_by_topic, topic_mapping\n",
    "\n",
    "def save_individual_topic_files(results_by_topic, output_dir=\"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/topic_data/\"):\n",
    "    \"\"\"\n",
    "    Save individual JSON files for each topic in original format\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    topics = ['science', 'technology', 'books', 'movies', 'music', 'politics', \n",
    "              'general entertainment', 'fashion', 'sports', 'food']\n",
    "    \n",
    "    for topic in topics:\n",
    "        # Check if topic has any data\n",
    "        has_data = any(len(results_by_topic[topic][source]['datastring']) > 0 \n",
    "                      for source in results_by_topic[topic])\n",
    "        \n",
    "        if has_data:\n",
    "            # Save in exact original format: {\"inlab\": {\"datastring\": [...]}, \"prolific\": {\"datastring\": [...]}}\n",
    "            topic_data = results_by_topic[topic]\n",
    "            filename = f\"{output_dir}compiled_{topic}.json\"\n",
    "            \n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(topic_data, f, indent=2)\n",
    "            \n",
    "            # Count participants for this topic\n",
    "            total_participants = sum(len(topic_data[source]['datastring']) \n",
    "                                   for source in topic_data)\n",
    "            print(f\"Saved {topic} data to {filename} ({total_participants} participants)\")\n",
    "\n",
    "def main(compiled_json_path, excel_path, output_path=\"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/compiled_by_topic.json\"):\n",
    "    \"\"\"\n",
    "    Main function to reorganize compiled JSON by topics\n",
    "    \n",
    "    Args:\n",
    "        compiled_json_path: Path to your existing compiled.json file\n",
    "        excel_path: Path to Excel file containing conversation_id and topic mapping\n",
    "        output_path: Path for output file (default: compiled_by_topic.json)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results_by_topic, topic_mapping = reorganize_by_topic(\n",
    "            compiled_json_path, excel_path, output_path\n",
    "        )\n",
    "        \n",
    "        # Optionally save individual topic files\n",
    "        save_individual_topic_files(results_by_topic)\n",
    "        \n",
    "        print(f\"\\nSuccess! Found topics: {list(set(topic_mapping.values()))}\")\n",
    "        return results_by_topic, topic_mapping\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Example usage:\n",
    "# results, mapping = main(\n",
    "#     compiled_json_path=\"your_compiled.json\",\n",
    "#     excel_path=\"your_topic_mapping.xlsx\"\n",
    "# )\n",
    "\n",
    "# Quick function to inspect what's in your compiled JSON\n",
    "def inspect_compiled_json(json_path):\n",
    "    \"\"\"\n",
    "    Quick inspection of compiled JSON structure\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(\"JSON Structure:\")\n",
    "    for key in data:\n",
    "        if isinstance(data[key], dict) and 'datastring' in data[key]:\n",
    "            participant_count = len(data[key]['datastring'])\n",
    "            print(f\"  {key}: {participant_count} participants\")\n",
    "            \n",
    "            # Look at first participant to see trial structure\n",
    "            if participant_count > 0:\n",
    "                first_participant = data[key]['datastring'][0]\n",
    "                trial_count = len(first_participant['data'])\n",
    "                print(f\"    - First participant has {trial_count} trials\")\n",
    "                \n",
    "                # Show a sample trial\n",
    "                if trial_count > 1:  # Skip the first dummy trial\n",
    "                    sample_trial = first_participant['data'][1]['trialdata']\n",
    "                    print(f\"    - Sample trial hit: {sample_trial.get('hit', 'N/A')}\")\n",
    "                    print(f\"    - Sample trial topic: {sample_trial.get('topic', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe6934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Structure:\n",
      "  inlab: 50 participants\n",
      "    - First participant has 21 trials\n",
      "    - Sample trial hit: 22\n",
      "    - Sample trial topic: 1\n",
      "  prolific: 102 participants\n",
      "    - First participant has 21 trials\n",
      "    - Sample trial hit: 207\n",
      "    - Sample trial topic: 5\n",
      "  mturk: 200 participants\n",
      "    - First participant has 21 trials\n",
      "    - Sample trial hit: 33\n",
      "    - Sample trial topic: 2\n",
      "  chatgpt: 15 participants\n",
      "    - First participant has 21 trials\n",
      "    - Sample trial hit: 3\n",
      "    - Sample trial topic: games\n"
     ]
    }
   ],
   "source": [
    "json_path = \"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults/compiled.json\"\n",
    "inspect_compiled_json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path=\"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/compiled_by_topic.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f59dffc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 299 conversation-topic mappings\n",
      "\n",
      "Processing inlab data...\n",
      "\n",
      "Processing prolific data...\n",
      "\n",
      "Processing mturk data...\n",
      "\n",
      "Processing chatgpt data...\n",
      "\n",
      "=========================\n",
      "REORGANIZATION COMPLETE\n",
      "Output saved to: /Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/compiled_by_topic.json\n",
      "=========================\n",
      "SUMMARY BY TOPIC:\n",
      "=========================\n",
      "SCIENCE: 304 participants (inlab: 42, prolific: 82, mturk: 165, chatgpt: 15)\n",
      "TECHNOLOGY: 270 participants (inlab: 42, prolific: 77, mturk: 151)\n",
      "BOOKS: 228 participants (inlab: 32, prolific: 62, mturk: 119, chatgpt: 15)\n",
      "MOVIES: 356 participants (inlab: 49, prolific: 99, mturk: 193, chatgpt: 15)\n",
      "MUSIC: 347 participants (inlab: 47, prolific: 97, mturk: 188, chatgpt: 15)\n",
      "POLITICS: 244 participants (inlab: 38, prolific: 69, mturk: 137)\n",
      "GENERAL ENTERTAINMENT: 350 participants (inlab: 48, prolific: 97, mturk: 190, chatgpt: 15)\n",
      "FASHION: 157 participants (inlab: 23, prolific: 42, mturk: 77, chatgpt: 15)\n",
      "SPORTS: 328 participants (inlab: 46, prolific: 92, mturk: 175, chatgpt: 15)\n",
      "FOOD: 310 participants (inlab: 42, prolific: 80, mturk: 173, chatgpt: 15)\n",
      "\n",
      "Original total participants: 367\n",
      "Saved science data to /Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/topic_data/compiled_science.json (304 participants)\n",
      "Saved technology data to /Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/topic_data/compiled_technology.json (270 participants)\n",
      "Saved books data to /Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/topic_data/compiled_books.json (228 participants)\n",
      "Saved movies data to /Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/topic_data/compiled_movies.json (356 participants)\n",
      "Saved music data to /Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/topic_data/compiled_music.json (347 participants)\n",
      "Saved politics data to /Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/topic_data/compiled_politics.json (244 participants)\n",
      "Saved general entertainment data to /Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/topic_data/compiled_general entertainment.json (350 participants)\n",
      "Saved fashion data to /Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/topic_data/compiled_fashion.json (157 participants)\n",
      "Saved sports data to /Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/topic_data/compiled_sports.json (328 participants)\n",
      "Saved food data to /Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/Plot/savedResults_elisa/topic_data/compiled_food.json (310 participants)\n",
      "\n",
      "Success! Found topics: ['sports', 'movies', 'nolabel', 'food', 'technology', 'science', 'books', 'politics', 'fashion', 'general entertainment', 'music']\n"
     ]
    }
   ],
   "source": [
    "results, topic_mapping = main(\n",
    "    compiled_json_path= json_path,\n",
    "    excel_path=\"/Users/elisapavarino/Documents/Work_Directory/Kreiman_Lab/Turing Test Paper/TuringGithub/conversation/conversation_task_for_revisions/code_dataanalsysis/record_conversations_paper.xlsx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d53bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############# for db file processing (in-lab)\n",
    "con = sqlite3.connect('./Data_all/Giorgia_all_inlab_online_1.db')\n",
    "# Load the data into a DataFrame\n",
    "df1 = pd.read_sql_query(\"SELECT * from TuringConversationalAI\", con)\n",
    "con = sqlite3.connect('./Data_all/Giorgia_all_inlab_online_2.db')\n",
    "# Load the data into a DataFrame\n",
    "df2 = pd.read_sql_query(\"SELECT * from TuringConversationalAI\", con)\n",
    "df = pd.concat([df1,df2], ignore_index=True)\n",
    "\n",
    "num_conv = 20\n",
    "key = 'mturk'\n",
    "for w in range(len(df)):\n",
    "    if df[\"mode\"][w] in [\"live\"] and df[\"endhit\"][w] is not None:\n",
    "        #data = json.loads(df.loc[w].at[\"datastring\"])\n",
    "        \n",
    "        workerdata = {'questiondata':{'gender':0,'age':0,'native':0,'education':0,'country':0},\n",
    "                          'data':[{'trialdata':{'hit':0,'topic':0,'hit_len':0,'Aclass':0,'rt':0,\n",
    "                                                'Agender': 0, 'Bgender': 0, 'conversationID': '.html'}}]}\n",
    "        \n",
    "        data = json.loads(df[\"datastring\"][w])\n",
    "        Wgender = data['questiondata']['gender']\n",
    "        Wage = data['questiondata']['age']\n",
    "        Wlanguage = data['questiondata']['native']\n",
    "        Wdegree = data['questiondata']['education']\n",
    "        Wfield = data['questiondata']['country']        \n",
    "        \n",
    "        workerdata['questiondata']['gender'] = Wgender\n",
    "        workerdata['questiondata']['age'] = Wage\n",
    "        workerdata['questiondata']['native'] = Wlanguage\n",
    "        workerdata['questiondata']['education'] = Wdegree\n",
    "        workerdata['questiondata']['country'] = Wfield       \n",
    "        \n",
    "        for c in range(num_conv):\n",
    "            \n",
    "            trialdata = {'trialdata':{'hit':0,'topic':0,'hit_len':0,'Aclass':0,'rt':0,\n",
    "                                                'Agender': 0, 'Bgender': 0, 'conversationID': '.html'}}\n",
    "            \n",
    "            #print(data['data'][1+c]['trialdata'])\n",
    "            # load responses to conv1\n",
    "            conv1_hit = data['data'][1+c]['trialdata']['hit']\n",
    "            conv1_len = data['data'][1+c]['trialdata']['hit_len']\n",
    "            conv1_combclass = data['data'][1+c]['trialdata']['Aclass']\n",
    "            conv1_topic = data['data'][1+c]['trialdata']['topic']\n",
    "            conv1_convname = 'static/dataset/conv'+str(conv1_hit)+'_len'+str(conv1_len)+'.html'\n",
    "            conv1_rt = data['data'][1+c]['trialdata']['rt'] # absolute\n",
    "            conv1_Agender = data['data'][1+c]['trialdata']['Agender']\n",
    "            conv1_Bgender = data['data'][1+c]['trialdata']['Bgender']\n",
    "            \n",
    "            trialdata['trialdata']['rt'] = conv1_rt\n",
    "            trialdata['trialdata']['hit'] = conv1_hit\n",
    "            trialdata['trialdata']['topic'] = conv1_topic\n",
    "            trialdata['trialdata']['hit_len'] = conv1_len\n",
    "            trialdata['trialdata']['Aclass'] = conv1_combclass\n",
    "            trialdata['trialdata']['Agender'] = conv1_Agender\n",
    "            trialdata['trialdata']['Bgender'] = conv1_Bgender\n",
    "            trialdata['trialdata']['conversationID'] = conv1_convname\n",
    "            \n",
    "            workerdata['data'].append(trialdata)\n",
    "        results[key]['datastring'].append(workerdata)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a93b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# for db file processing (chatGPT)\n",
    "con = sqlite3.connect('./Data_all/Mranmay_inlab_ChatGPTOnlyWoGenders.db')\n",
    "# Load the data into a DataFrame\n",
    "df1 = pd.read_sql_query(\"SELECT * from TuringConversationalAI\", con)\n",
    "df = pd.concat([df1], ignore_index=True)\n",
    "\n",
    "num_conv = 20\n",
    "key = 'chatgpt'\n",
    "for w in range(len(df)):\n",
    "    \n",
    "    if df[\"mode\"][w] in [\"debug\"] and df[\"endhit\"][w] is not None:\n",
    "        #data = json.loads(df.loc[w].at[\"datastring\"])\n",
    "               \n",
    "        workerdata = {'questiondata':{'gender':0,'age':0,'native':0,'education':0,'country':0},\n",
    "                          'data':[{'trialdata':{'hit':0,'topic':0,'hit_len':0,'Aclass':0,'rt':0,\n",
    "                                                 'conversationID': '.html'}}]}\n",
    "        \n",
    "        data = json.loads(df[\"datastring\"][w])\n",
    "        \n",
    "        #print(data)\n",
    "        \n",
    "        Wgender = data['questiondata']['gender']\n",
    "        Wage = data['questiondata']['age']\n",
    "        Wlanguage = data['questiondata']['native']\n",
    "        Wdegree = data['questiondata']['education']\n",
    "        Wfield = data['questiondata']['country']        \n",
    "        \n",
    "        workerdata['questiondata']['gender'] = Wgender\n",
    "        workerdata['questiondata']['age'] = Wage\n",
    "        workerdata['questiondata']['native'] = Wlanguage\n",
    "        workerdata['questiondata']['education'] = Wdegree\n",
    "        workerdata['questiondata']['country'] = Wfield       \n",
    "        \n",
    "        for c in range(num_conv):\n",
    "            #print(c)\n",
    "            trialdata = {'trialdata':{'hit':0,'topic':0,'hit_len':0,'Aclass':0,'rt':0,\n",
    "                                                'Agender': 0, 'Bgender': 0, 'conversationID': '.html'}}\n",
    "            \n",
    "            #print(data['data'][1+c]['trialdata'])\n",
    "            # load responses to conv1\n",
    "            #print(data['data'][2])\n",
    "            \n",
    "            conv1_hit = data['data'][1+c]['trialdata']['hit']\n",
    "            conv1_len = data['data'][1+c]['trialdata']['hit_len']\n",
    "            conv1_combclass = data['data'][1+c]['trialdata']['Aclass']\n",
    "            conv1_topic = data['data'][1+c]['trialdata']['topic']\n",
    "            conv1_convname = 'Mranmey_chatgpt_dataset_woGender/conv'+str(conv1_hit)+'_len'+str(conv1_len)+'.html'\n",
    "            conv1_rt = data['data'][1+c]['trialdata']['rt'] # absolute\n",
    "            #conv1_Agender = data['data'][1+c]['trialdata']['Agender']\n",
    "            #conv1_Bgender = data['data'][1+c]['trialdata']['Bgender']\n",
    "            \n",
    "            trialdata['trialdata']['rt'] = conv1_rt\n",
    "            trialdata['trialdata']['hit'] = conv1_hit\n",
    "            trialdata['trialdata']['topic'] = conv1_topic\n",
    "            trialdata['trialdata']['hit_len'] = conv1_len\n",
    "            trialdata['trialdata']['Aclass'] = conv1_combclass\n",
    "            #trialdata['trialdata']['Agender'] = conv1_Agender\n",
    "            #trialdata['trialdata']['Bgender'] = conv1_Bgender\n",
    "            trialdata['trialdata']['conversationID'] = conv1_convname\n",
    "            \n",
    "            workerdata['data'].append(trialdata)\n",
    "        results[key]['datastring'].append(workerdata)\n",
    "        \n",
    "print(\"=========================\")\n",
    "print(\"number of in-lab pariticpants:\")\n",
    "print(len(results['inlab']['datastring']))\n",
    "countTrial = 0\n",
    "for work in range(len(results['inlab']['datastring'])):\n",
    "    countTrial = countTrial + len(results['inlab']['datastring'][work]['data'])\n",
    "print(countTrial)\n",
    "\n",
    "print(\"number of prolific pariticpants:\")\n",
    "print(len(results['prolific']['datastring']))\n",
    "countTrial = 0\n",
    "for work in range(len(results['prolific']['datastring'])):\n",
    "    countTrial = countTrial + len(results['prolific']['datastring'][work]['data'])\n",
    "print(countTrial)\n",
    "\n",
    "print(\"number of mturk pariticpants:\")\n",
    "print(len(results['mturk']['datastring']))\n",
    "countTrial = 0\n",
    "for work in range(len(results['mturk']['datastring'])):\n",
    "    countTrial = countTrial + len(results['mturk']['datastring'][work]['data'])\n",
    "print(countTrial)\n",
    "\n",
    "print(\"number of chatgpt pariticpants:\")\n",
    "print(len(results['chatgpt']['datastring']))\n",
    "countTrial = 0\n",
    "for work in range(len(results['chatgpt']['datastring'])):\n",
    "    countTrial = countTrial + len(results['chatgpt']['datastring'][work]['data'])\n",
    "print(countTrial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed3137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation completed! Results are saved to folder!\n"
     ]
    }
   ],
   "source": [
    "#save the compiled results as jason\n",
    "with open(\"./savedResults_elisa/compiled_prolific.json\", \"w\") as fp:\n",
    "    json.dump(results,fp) \n",
    "    \n",
    "print('Compilation completed! Results are saved to folder!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "1050 + 2142 + 4200 + 315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a52ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
